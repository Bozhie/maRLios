{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import gym\n",
    "import gym_super_mario_bros\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "from gym_super_mario_bros import SuperMarioBrosEnv\n",
    "from tqdm import tqdm\n",
    "import pickle \n",
    "import gym\n",
    "import numpy as np\n",
    "import collections \n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import datetime\n",
    "import json\n",
    "from toolkit.gym_env import *\n",
    "from toolkit.action_utils import *\n",
    "import pandas as pd # here if you need it\n",
    "\n",
    "# models CHANGE FOR YOUR MODEL\n",
    "from toolkit.train_marlios_rnn import make_env #NOTE your make env may not be here\n",
    "from toolkit.marlios_rnn import *\n",
    "##################################\n",
    "\n",
    "from toolkit.constants import *\n",
    "from toolkit.train_test_samples import *\n",
    "import warnings\n",
    "from toolkit.statistics import *\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 32/100 [19:29<41:25, 36.55s/it] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 36\u001b[0m\n\u001b[1;32m     33\u001b[0m flags_got, pipe2, pipe3, pipe4 \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m\n\u001b[1;32m     35\u001b[0m \u001b[39mfor\u001b[39;00m ep \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(num_episodes)):\n\u001b[0;32m---> 36\u001b[0m     stats \u001b[39m=\u001b[39m get_stats_run(agent, env) \u001b[39m#NOTE: my validate run is coming from statistics.py\u001b[39;00m\n\u001b[1;32m     37\u001b[0m     total_rewards\u001b[39m.\u001b[39mappend(stats[\u001b[39m'\u001b[39m\u001b[39mtotal_reward\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     38\u001b[0m     x_position\u001b[39m.\u001b[39mappend(stats[\u001b[39m'\u001b[39m\u001b[39mx_pos\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[0;32m~/Desktop/CSCI_566/maRLios/toolkit/statistics.py:29\u001b[0m, in \u001b[0;36mget_stats_run\u001b[0;34m(agent, env)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m terminal:\n\u001b[1;32m     26\u001b[0m     \u001b[39m# compute index into ACTION_SPACE of our action\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     step_action \u001b[39m=\u001b[39m ACTION_TO_INDEX[action]\n\u001b[0;32m---> 29\u001b[0m     state_next, cur_reward, terminal, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(step_action)\n\u001b[1;32m     30\u001b[0m     total_reward \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m cur_reward\n\u001b[1;32m     31\u001b[0m     reward \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m cur_reward\n",
      "File \u001b[0;32m~/Desktop/CSCI_566/env/lib/python3.10/site-packages/nes_py/wrappers/joypad_space.py:74\u001b[0m, in \u001b[0;36mJoypadSpace.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[39mTake a step using the given action.\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     71\u001b[0m \n\u001b[1;32m     72\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[39m# take the step and record the output\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_action_map[action])\n",
      "File \u001b[0;32m~/Desktop/CSCI_566/env/lib/python3.10/site-packages/gym/core.py:483\u001b[0m, in \u001b[0;36mObservationWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[1;32m    482\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Returns a modified observation using :meth:`self.observation` after calling :meth:`env.step`.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 483\u001b[0m     step_returns \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m    484\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(step_returns) \u001b[39m==\u001b[39m \u001b[39m5\u001b[39m:\n\u001b[1;32m    485\u001b[0m         observation, reward, terminated, truncated, info \u001b[39m=\u001b[39m step_returns\n",
      "File \u001b[0;32m~/Desktop/CSCI_566/env/lib/python3.10/site-packages/gym/core.py:483\u001b[0m, in \u001b[0;36mObservationWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[1;32m    482\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Returns a modified observation using :meth:`self.observation` after calling :meth:`env.step`.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 483\u001b[0m     step_returns \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m    484\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(step_returns) \u001b[39m==\u001b[39m \u001b[39m5\u001b[39m:\n\u001b[1;32m    485\u001b[0m         observation, reward, terminated, truncated, info \u001b[39m=\u001b[39m step_returns\n",
      "File \u001b[0;32m~/Desktop/CSCI_566/env/lib/python3.10/site-packages/gym/core.py:483\u001b[0m, in \u001b[0;36mObservationWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[1;32m    482\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Returns a modified observation using :meth:`self.observation` after calling :meth:`env.step`.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 483\u001b[0m     step_returns \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m    484\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(step_returns) \u001b[39m==\u001b[39m \u001b[39m5\u001b[39m:\n\u001b[1;32m    485\u001b[0m         observation, reward, terminated, truncated, info \u001b[39m=\u001b[39m step_returns\n",
      "File \u001b[0;32m~/Desktop/CSCI_566/env/lib/python3.10/site-packages/gym/core.py:489\u001b[0m, in \u001b[0;36mObservationWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    488\u001b[0m     observation, reward, done, info \u001b[39m=\u001b[39m step_returns\n\u001b[0;32m--> 489\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mobservation(observation), reward, done, info\n",
      "File \u001b[0;32m~/Desktop/CSCI_566/maRLios/toolkit/gym_env.py:56\u001b[0m, in \u001b[0;36mProcessFrame84.observation\u001b[0;34m(self, obs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mobservation\u001b[39m(\u001b[39mself\u001b[39m, obs):\n\u001b[0;32m---> 56\u001b[0m     \u001b[39mreturn\u001b[39;00m ProcessFrame84\u001b[39m.\u001b[39;49mprocess(obs)\n",
      "File \u001b[0;32m~/Desktop/CSCI_566/maRLios/toolkit/gym_env.py:65\u001b[0m, in \u001b[0;36mProcessFrame84.process\u001b[0;34m(frame)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mUnknown resolution.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     64\u001b[0m img \u001b[39m=\u001b[39m img[:, :, \u001b[39m0\u001b[39m] \u001b[39m*\u001b[39m \u001b[39m0.299\u001b[39m \u001b[39m+\u001b[39m img[:, :, \u001b[39m1\u001b[39m] \u001b[39m*\u001b[39m \u001b[39m0.587\u001b[39m \u001b[39m+\u001b[39m img[:, :, \u001b[39m2\u001b[39m] \u001b[39m*\u001b[39m \u001b[39m0.114\u001b[39m\n\u001b[0;32m---> 65\u001b[0m resized_screen \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39;49mresize(img, (\u001b[39m84\u001b[39;49m, \u001b[39m110\u001b[39;49m), interpolation\u001b[39m=\u001b[39;49mcv2\u001b[39m.\u001b[39;49mINTER_AREA)\n\u001b[1;32m     66\u001b[0m x_t \u001b[39m=\u001b[39m resized_screen[\u001b[39m18\u001b[39m:\u001b[39m102\u001b[39m, :]\n\u001b[1;32m     67\u001b[0m x_t \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mreshape(x_t, [\u001b[39m84\u001b[39m, \u001b[39m84\u001b[39m, \u001b[39m1\u001b[39m])\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# create the environment (don't change)\n",
    "mario_env='SuperMarioBros-1-1-v0'\n",
    "env = gym.make(mario_env)\n",
    "env = make_env(env, ACTION_SPACE) # (this is defined in my train_marlios_rnn.py, make sure you import yours)\n",
    "\n",
    "####################\n",
    "# load the agent (change depending on your model architecture)\n",
    "agent = DQNAgent(\n",
    "                state_space=env.observation_space.shape,\n",
    "                action_space=TEST_SET, #NOTE CHANGE THIS depending on the stats you track\n",
    "                gamma=0.9,\n",
    "                max_memory_size=3000, # this doesnt matter here\n",
    "                batch_size=64, # this doesnt matter here\n",
    "                lr=0,\n",
    "                dropout=None,\n",
    "                exploration_max=0,\n",
    "                exploration_min=0,\n",
    "                exploration_decay=0.9995,\n",
    "                double_dq=True,\n",
    "                pretrained=True,\n",
    "                run_id='marlios_rnn_test', #NOTE CHANGE THIS\n",
    "                n_actions=64,  \n",
    "                training_stage='test', # your model may not have this parameter, make sure if you're running the rnn its 'train', 'test' or 'val' depending on action space\n",
    "                device='cpu', # probably fine to leave this\n",
    "                add_sufficient=True, # your action utils may not require this (or your model)\n",
    "                hidden_shape=64 # must be the same for what we trained the model on (can reference wandb)\n",
    "                )\n",
    "\n",
    "# setup lists of stats we are tracking\n",
    "num_episodes = 100\n",
    "total_rewards = []\n",
    "x_position = []\n",
    "flags_got, pipe2, pipe3, pipe4 = 0, 0, 0, 0\n",
    "\n",
    "for ep in tqdm(range(num_episodes)):\n",
    "    stats = get_stats_run(agent, env) #NOTE: my validate run is coming from statistics.py\n",
    "    total_rewards.append(stats['total_reward'])\n",
    "    x_position.append(stats['x_pos'])\n",
    "    flags_got += stats['flag_get']\n",
    "    pipe2 += stats['pipe2']\n",
    "    pipe3 += stats['pipe3']\n",
    "    pipe4 +=  stats['pipe4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_statistics(training_stage, total_rewards, x_position, flags, pipes, num_episodes):\n",
    "    pipe2, pipe3, pipe4 = pipes\n",
    "\n",
    "    avg_total_rewards = np.mean(total_rewards)\n",
    "    stddev_rewards = np.std(total_rewards)\n",
    "\n",
    "    avg_xpos = np.mean(x_position)\n",
    "    stddev_xpos = np.mean(x_position)\n",
    "\n",
    "    print(\"Statistics for \", training_stage)\n",
    "    print(f\"Avg Total Rewards: {avg_total_rewards:.3f}\")\n",
    "    print(f\"Std Dev Rewards: {stddev_rewards:.3f}\")\n",
    "    print(f\"# of Wins: {flags_got}\")\n",
    "    print(f\"% Pipe 1 {100*pipe2/num_episodes:.3f}\")\n",
    "    print(f\"% Pipe 2 {100*pipe3/num_episodes:.3f}\")\n",
    "    print(f\"% Pipe 3 {100*pipe4/num_episodes:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics for  TEST (holdout)\n",
      "Avg Total Rewards: 462.260\n",
      "Std Dev Rewards: 270.286\n",
      "# of Wins: 0\n",
      "% Pipe 1 83.000\n",
      "% Pipe 2 22.000\n",
      "% Pipe 3 1.000\n"
     ]
    }
   ],
   "source": [
    "print_statistics(training_stage=\"TEST (holdout)\", \n",
    "                 total_rewards=total_rewards,\n",
    "                 x_position=x_position,\n",
    "                 flags=flags_got,\n",
    "                 pipes=(pipe2, pipe3, pipe4),\n",
    "                 num_episodes=num_episodes\n",
    "                 )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
