{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #only run once\n",
    "# #!pip install nes-py==0.2.6\n",
    "# !brew update\n",
    "# !brew install ffmpeg\n",
    "# !brew install libsm\n",
    "# !brew install libxext\n",
    "# !brew install mesa\n",
    "# !pip install opencv-python\n",
    "# !pip install gym-super-mario-bros\n",
    "# !pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import gym\n",
    "import gym_super_mario_bros\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "from gym_super_mario_bros import SuperMarioBrosEnv\n",
    "from tqdm import tqdm\n",
    "import pickle \n",
    "import gym\n",
    "import numpy as np\n",
    "import collections \n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import datetime\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from toolkit.gym_env import *\n",
    "from toolkit.action_utils import *\n",
    "from toolkit.marlios_model import *\n",
    "from toolkit.constants import *\n",
    "\n",
    "CONSECUTIVE_ACTIONS = 2\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_state(env, ep=0, info=\"\"):\n",
    "    plt.figure(3)\n",
    "    plt.clf()\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    plt.title(\"Episode: %d %s\" % (ep, info))\n",
    "    plt.axis('off')\n",
    "\n",
    "    # display.clear_output(wait=True)\n",
    "    # display.display(plt.gcf())\n",
    "    display(plt.gcf(), clear=True)\n",
    "\n",
    "def make_env(env, actions=ACTION_SPACE):\n",
    "    env = MaxAndSkipEnv(env, skip=2) # I am testing out fewer fram repetitions for our two actions modelling\n",
    "    env = ProcessFrame84(env)\n",
    "    env = ImageToPyTorch(env)\n",
    "    env = BufferWrapper(env, 4)\n",
    "    env = ScaledFloatFrame(env)\n",
    "    return JoypadSpace(env, actions)\n",
    "\n",
    "def generate_epoch_time_id():\n",
    "    epoch_time = int(time.time())\n",
    "    return str(epoch_time)\n",
    "\n",
    "def save_checkpoint(agent, total_rewards, terminal_info, run_id):\n",
    "    with open(f\"ending_position-{run_id}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(agent.ending_position, f)\n",
    "    with open(f\"num_in_queue-{run_id}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(agent.num_in_queue, f)\n",
    "    with open(f\"total_rewards-{run_id}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(total_rewards, f)\n",
    "    with open(f\"terminal_info-{run_id}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(terminal_info, f)\n",
    "    if agent.double_dq:\n",
    "        torch.save(agent.local_net.state_dict(), f\"dq1-{run_id}.pt\")\n",
    "        torch.save(agent.target_net.state_dict(), f\"dq2-{run_id}.pt\")\n",
    "    else:\n",
    "        torch.save(agent.dqn.state_dict(), f\"dq-{run_id}.pt\")  \n",
    "\n",
    "\n",
    "# define the run function with some helpful debugging stats\n",
    "\n",
    "def run(training_mode=True, pretrained=False, lr=0.0001, gamma=0.90, exploration_decay=0.995, exploration_min=0.02, ep_per_stat = 100,\n",
    "        mario_env='SuperMarioBros-1-1-v0', action_space=TWO_ACTIONS_SET, num_episodes=1000, run_id=None, n_actions=5):\n",
    "   \n",
    "    run_id = run_id or generate_epoch_time_id()\n",
    "    fh = open(f'progress-{run_id}.txt', 'a')\n",
    "    env = gym.make(mario_env)\n",
    "    env = make_env(env, ACTION_SPACE)\n",
    "    # observation_space = env.observation_space.shape # not using this anymore\n",
    "\n",
    "    #todo: add agent params as a setting/create different agents in diff functions to run \n",
    "\n",
    "    agent = DQNAgent(\n",
    "                     action_space=action_space,\n",
    "                     max_memory_size=30000,\n",
    "                     batch_size=32,\n",
    "                     gamma=gamma,\n",
    "                     lr=lr,\n",
    "                     dropout=0.,\n",
    "                     exploration_max=1,\n",
    "                     exploration_min=exploration_min,\n",
    "                     exploration_decay=exploration_decay,\n",
    "                     double_dq=True,\n",
    "                     pretrained=pretrained,\n",
    "                     run_id=run_id,\n",
    "                     n_actions=n_actions)\n",
    "    \n",
    "    \n",
    "    # num_episodes = 10\n",
    "    env.reset()\n",
    "    total_rewards = []\n",
    "    total_info = []\n",
    "    \n",
    "    for ep_num in tqdm(range(num_episodes)):\n",
    "        state = env.reset()[-1] # take the final dimension of shape (4, 84, 84) leaving shape (84, 84) \n",
    "        state = torch.Tensor([state]).unsqueeze(0) # converts (1, 84, 84) to (1, 1, 84, 84)\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "\n",
    "        action_freq = {}\n",
    "        while True:\n",
    "            if not training_mode:\n",
    "                show_state(env, ep_num)\n",
    "\n",
    "\n",
    "            two_actions_index = agent.act(state)\n",
    "            two_actions_vector = agent.cur_action_space[0, two_actions_index[0]]\n",
    "            two_actions = vec_to_action(two_actions_vector.cpu()) # tuple of actions\n",
    "\n",
    "            # debugging info\n",
    "            key = \" | \".join([\",\".join(i) for i in two_actions])\n",
    "            if key in action_freq:\n",
    "                action_freq[key] += 1\n",
    "            else:\n",
    "                action_freq[key] = 1\n",
    "            \n",
    "            steps += 1\n",
    "            reward = 0\n",
    "            info = None\n",
    "            terminal = False\n",
    "            for action in two_actions: \n",
    "                if not terminal:\n",
    "                    # compute index into ACTION_SPACE of our action\n",
    "                    step_action = ACTION_TO_INDEX[action]\n",
    "\n",
    "                    state_next, cur_reward, terminal, info = env.step(step_action)\n",
    "                    total_reward += cur_reward\n",
    "                    reward += cur_reward\n",
    "                    \n",
    "            state_next = torch.Tensor([state_next[-1]]).unsqueeze(0)\n",
    "            reward = torch.tensor([reward]).unsqueeze(0)        \n",
    "            terminal = torch.tensor([int(terminal)]).unsqueeze(0)\n",
    "            \n",
    "            if training_mode:\n",
    "                agent.remember(state, two_actions_index, reward, state_next, terminal)\n",
    "                agent.experience_replay()\n",
    "            \n",
    "            state = state_next\n",
    "            if terminal:\n",
    "                break\n",
    "\n",
    "        total_info.append(info)\n",
    "        total_rewards.append(total_reward)\n",
    "\n",
    "        if training_mode and (ep_num % ep_per_stat) == 0:\n",
    "            save_checkpoint(agent, total_rewards, total_info, run_id)\n",
    "\n",
    "        with open(f'total_reward-{run_id}.txt', 'a') as f:\n",
    "            f.write(\"Total reward after episode {} is {}\\n\".format(ep_num + 1, total_rewards[-1]))\n",
    "            if (ep_num%100 == 0):\n",
    "                f.write(\"==================\\n\")\n",
    "                f.write(\"{} current time at episode {}\\n\".format(datetime.datetime.now(), ep_num+1))\n",
    "                f.write(\"==================\\n\")\n",
    "            #print(\"Total reward after episode {} is {}\".format(ep_num + 1, total_rewards[-1]))\n",
    "            num_episodes += 1\n",
    "        \n",
    "        with open(f'actions_chosen-{run_id}.txt', 'a') as f:\n",
    "            f.write(\"Action Frequencies for Episode {}, Exploration = {:4f}\\n\".format(ep_num + 1, agent.exploration_rate))\n",
    "            f.write(json.dumps(action_freq) + \"\\n\\n\")\n",
    "    \n",
    "    if training_mode:\n",
    "        save_checkpoint(agent, total_rewards, run_id)\n",
    "    \n",
    "    env.close()\n",
    "    fh.close()\n",
    "    \n",
    "    if num_episodes > ep_per_stat:\n",
    "        plt.title(\"Episodes trained vs. Average Rewards (per 500 eps)\")\n",
    "        plt.plot([0 for _ in range(ep_per_stat)] + \n",
    "                 np.convolve(total_rewards, np.ones((ep_per_stat,))/ep_per_stat, mode=\"valid\").tolist())\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [1:08:01<00:00,  4.08s/it]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "save_checkpoint() missing 1 required positional argument: 'run_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[97], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m run(training_mode\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, \n\u001b[1;32m      2\u001b[0m     pretrained\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m      3\u001b[0m     ep_per_stat\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, \n\u001b[1;32m      4\u001b[0m     lr\u001b[39m=\u001b[39;49m\u001b[39m0.000005\u001b[39;49m,\n\u001b[1;32m      5\u001b[0m     exploration_min\u001b[39m=\u001b[39;49m\u001b[39m0.1\u001b[39;49m,\n\u001b[1;32m      6\u001b[0m     exploration_decay\u001b[39m=\u001b[39;49m\u001b[39m0.9995\u001b[39;49m, \n\u001b[1;32m      7\u001b[0m     action_space\u001b[39m=\u001b[39;49mSIMPLE_MOVEMENT,\n\u001b[1;32m      8\u001b[0m       n_actions\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(SIMPLE_MOVEMENT)\u001b[39m+\u001b[39;49m\u001b[39m2\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[96], line 142\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(training_mode, pretrained, lr, gamma, exploration_decay, exploration_min, ep_per_stat, mario_env, action_space, num_episodes, run_id, n_actions)\u001b[0m\n\u001b[1;32m    139\u001b[0m         f\u001b[39m.\u001b[39mwrite(json\u001b[39m.\u001b[39mdumps(action_freq) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    141\u001b[0m \u001b[39mif\u001b[39;00m training_mode:\n\u001b[0;32m--> 142\u001b[0m     save_checkpoint(agent, total_rewards, run_id)\n\u001b[1;32m    144\u001b[0m env\u001b[39m.\u001b[39mclose()\n\u001b[1;32m    145\u001b[0m fh\u001b[39m.\u001b[39mclose()\n",
      "\u001b[0;31mTypeError\u001b[0m: save_checkpoint() missing 1 required positional argument: 'run_id'"
     ]
    }
   ],
   "source": [
    "run(training_mode=True, \n",
    "    pretrained=False,\n",
    "    ep_per_stat=100, \n",
    "    lr=0.000005,\n",
    "    exploration_min=0.1,\n",
    "    exploration_decay=0.9995, \n",
    "    action_space=SIMPLE_MOVEMENT,\n",
    "    n_actions=len(SIMPLE_MOVEMENT)+2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/1000 [04:08<17:09:22, 62.01s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[74], line 78\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m terminal:\n\u001b[1;32m     75\u001b[0m     \u001b[39m# compute index into ACTION_SPACE of our action\u001b[39;00m\n\u001b[1;32m     76\u001b[0m     step_action \u001b[39m=\u001b[39m ACTION_TO_INDEX[action]\n\u001b[0;32m---> 78\u001b[0m     state_next, cur_reward, terminal, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(step_action)\n\u001b[1;32m     79\u001b[0m     total_reward \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m cur_reward\n\u001b[1;32m     80\u001b[0m     reward \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m cur_reward\n",
      "File \u001b[0;32m~/Desktop/CSCI_566/env/lib/python3.10/site-packages/nes_py/wrappers/joypad_space.py:74\u001b[0m, in \u001b[0;36mJoypadSpace.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[39mTake a step using the given action.\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     71\u001b[0m \n\u001b[1;32m     72\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[39m# take the step and record the output\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_action_map[action])\n",
      "File \u001b[0;32m~/Desktop/CSCI_566/env/lib/python3.10/site-packages/gym/core.py:483\u001b[0m, in \u001b[0;36mObservationWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[1;32m    482\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Returns a modified observation using :meth:`self.observation` after calling :meth:`env.step`.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 483\u001b[0m     step_returns \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m    484\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(step_returns) \u001b[39m==\u001b[39m \u001b[39m5\u001b[39m:\n\u001b[1;32m    485\u001b[0m         observation, reward, terminated, truncated, info \u001b[39m=\u001b[39m step_returns\n",
      "File \u001b[0;32m~/Desktop/CSCI_566/env/lib/python3.10/site-packages/gym/core.py:483\u001b[0m, in \u001b[0;36mObservationWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[1;32m    482\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Returns a modified observation using :meth:`self.observation` after calling :meth:`env.step`.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 483\u001b[0m     step_returns \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m    484\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(step_returns) \u001b[39m==\u001b[39m \u001b[39m5\u001b[39m:\n\u001b[1;32m    485\u001b[0m         observation, reward, terminated, truncated, info \u001b[39m=\u001b[39m step_returns\n",
      "    \u001b[0;31m[... skipping similar frames: ObservationWrapper.step at line 483 (1 times)]\u001b[0m\n",
      "File \u001b[0;32m~/Desktop/CSCI_566/env/lib/python3.10/site-packages/gym/core.py:483\u001b[0m, in \u001b[0;36mObservationWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[1;32m    482\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Returns a modified observation using :meth:`self.observation` after calling :meth:`env.step`.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 483\u001b[0m     step_returns \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m    484\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(step_returns) \u001b[39m==\u001b[39m \u001b[39m5\u001b[39m:\n\u001b[1;32m    485\u001b[0m         observation, reward, terminated, truncated, info \u001b[39m=\u001b[39m step_returns\n",
      "File \u001b[0;32m~/Desktop/CSCI_566/maRLios/toolkit/gym_env.py:27\u001b[0m, in \u001b[0;36mMaxAndSkipEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     25\u001b[0m done \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_skip):\n\u001b[0;32m---> 27\u001b[0m     obs, reward, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     28\u001b[0m     \u001b[39m# obs, reward, done, truncation, info = self.env.step(action)\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_obs_buffer\u001b[39m.\u001b[39mappend(obs)\n",
      "File \u001b[0;32m~/Desktop/CSCI_566/env/lib/python3.10/site-packages/gym/wrappers/time_limit.py:60\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[1;32m     49\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \n\u001b[1;32m     51\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[39m        \"TimeLimit.truncated\"=False if the environment terminated\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m     59\u001b[0m     observation, reward, terminated, truncated, info \u001b[39m=\u001b[39m step_api_compatibility(\n\u001b[0;32m---> 60\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action),\n\u001b[1;32m     61\u001b[0m         \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     62\u001b[0m     )\n\u001b[1;32m     63\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/Desktop/CSCI_566/env/lib/python3.10/site-packages/gym/wrappers/order_enforcing.py:37\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset:\n\u001b[1;32m     36\u001b[0m     \u001b[39mraise\u001b[39;00m ResetNeeded(\u001b[39m\"\u001b[39m\u001b[39mCannot call env.step() before calling env.reset()\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[0;32m~/Desktop/CSCI_566/env/lib/python3.10/site-packages/gym/wrappers/step_api_compatibility.py:52\u001b[0m, in \u001b[0;36mStepAPICompatibility.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[1;32m     44\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Steps through the environment, returning 5 or 4 items depending on `new_step_api`.\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \n\u001b[1;32m     46\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39m        (observation, reward, terminated, truncated, info) or (observation, reward, done, info)\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m     step_returns \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     53\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnew_step_api:\n\u001b[1;32m     54\u001b[0m         \u001b[39mreturn\u001b[39;00m step_to_new_api(step_returns)\n",
      "File \u001b[0;32m~/Desktop/CSCI_566/env/lib/python3.10/site-packages/gym/wrappers/env_checker.py:39\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[39mreturn\u001b[39;00m env_step_passive_checker(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv, action)\n\u001b[1;32m     38\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 39\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[0;32m~/Desktop/CSCI_566/env/lib/python3.10/site-packages/nes_py/nes_env.py:306\u001b[0m, in \u001b[0;36mNESEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdone \u001b[39m=\u001b[39m \u001b[39mbool\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_done())\n\u001b[1;32m    305\u001b[0m \u001b[39m# get the info for this step\u001b[39;00m\n\u001b[0;32m--> 306\u001b[0m info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_info()\n\u001b[1;32m    307\u001b[0m \u001b[39m# call the after step callback\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_did_step(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdone)\n",
      "File \u001b[0;32m~/Desktop/CSCI_566/env/lib/python3.10/site-packages/gym_super_mario_bros/smb_env.py:416\u001b[0m, in \u001b[0;36mSuperMarioBrosEnv._get_info\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_info\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    406\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return the info after a step occurs\"\"\"\u001b[39;00m\n\u001b[1;32m    407\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mdict\u001b[39m(\n\u001b[1;32m    408\u001b[0m         coins\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_coins,\n\u001b[1;32m    409\u001b[0m         flag_get\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flag_get,\n\u001b[1;32m    410\u001b[0m         life\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_life,\n\u001b[1;32m    411\u001b[0m         score\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_score,\n\u001b[1;32m    412\u001b[0m         stage\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stage,\n\u001b[1;32m    413\u001b[0m         status\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_player_status,\n\u001b[1;32m    414\u001b[0m         time\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time,\n\u001b[1;32m    415\u001b[0m         world\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_world,\n\u001b[0;32m--> 416\u001b[0m         x_pos\u001b[39m=\u001b[39m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_x_position,\n\u001b[1;32m    417\u001b[0m         y_pos\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_y_position,\n\u001b[1;32m    418\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/CSCI_566/env/lib/python3.10/site-packages/gym_super_mario_bros/smb_env.py:138\u001b[0m, in \u001b[0;36mSuperMarioBrosEnv._x_position\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return the number of remaining lives.\"\"\"\u001b[39;00m\n\u001b[1;32m    136\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mram[\u001b[39m0x075a\u001b[39m]\n\u001b[0;32m--> 138\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[1;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_x_position\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    140\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return the current horizontal position.\"\"\"\u001b[39;00m\n\u001b[1;32m    141\u001b[0m     \u001b[39m# add the current page 0x6d to the current x\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training_mode=True\n",
    "pretrained=False\n",
    "lr=0.000005\n",
    "gamma=0.90\n",
    "exploration_decay=0.995\n",
    "exploration_min=0.02\n",
    "ep_per_stat = 100\n",
    "mario_env='SuperMarioBros-1-1-v0'\n",
    "action_space=SIMPLE_MOVEMENT\n",
    "num_episodes=1000\n",
    "run_id=None\n",
    "n_actions=len(SIMPLE_MOVEMENT) + 2\n",
    "consecutiveActions = 2\n",
    "\n",
    "run_id = run_id or generate_epoch_time_id()\n",
    "fh = open(f'progress-{run_id}.txt', 'a')\n",
    "env = gym.make(mario_env)\n",
    "#env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
    "\n",
    "#env = make_env(env)  # Wraps the environment so that frames are grayscale \n",
    "#env = SuperMarioBrosEnv()\n",
    "env = make_env(env, ACTION_SPACE)\n",
    "# observation_space = env.observation_space.shape # not using this anymore\n",
    "\n",
    "#todo: add agent params as a setting/create different agents in diff functions to run \n",
    "\n",
    "agent = DQNAgent(\n",
    "                    action_space=action_space,\n",
    "                    max_memory_size=30000,\n",
    "                    batch_size=64,\n",
    "                    gamma=gamma,\n",
    "                    lr=lr,\n",
    "                    dropout=0.,\n",
    "                    exploration_max=1,\n",
    "                    exploration_min=exploration_min,\n",
    "                    exploration_decay=exploration_decay,\n",
    "                    double_dq=True,\n",
    "                    pretrained=pretrained,\n",
    "                    run_id=run_id,\n",
    "                    n_actions=n_actions)\n",
    "\n",
    "\n",
    "# num_episodes = 10\n",
    "env.reset()\n",
    "total_rewards = []\n",
    "total_info = []\n",
    "\n",
    "for ep_num in tqdm(range(num_episodes)):\n",
    "    state = env.reset()[-1] # take the final dimension of shape (4, 84, 84) leaving shape (84, 84) \n",
    "    state = torch.Tensor([state]).unsqueeze(0) # converts (1, 84, 84) to (1, 1, 84, 84)\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    while True:\n",
    "        if not training_mode:\n",
    "            show_state(env, ep_num)\n",
    "\n",
    "\n",
    "        two_actions_index = agent.act(state)\n",
    "        # print()\n",
    "        # print(two_actions_index.float())\n",
    "        two_actions_vector = agent.cur_action_space[0, two_actions_index[0]]\n",
    "        # print(two_actions_vector)\n",
    "\n",
    "        two_actions = vec_to_action(two_actions_vector.cpu()) # tuple of actions\n",
    "\n",
    "        if ep_num % 5 == 0 and ep_num != 0:\n",
    "            print(two_actions)\n",
    "\n",
    "        steps += 1\n",
    "        reward = 0\n",
    "        info = None\n",
    "        terminal = False\n",
    "        for action in two_actions: \n",
    "            if not terminal:\n",
    "                # compute index into ACTION_SPACE of our action\n",
    "                step_action = ACTION_TO_INDEX[action]\n",
    "\n",
    "                state_next, cur_reward, terminal, info = env.step(step_action)\n",
    "                total_reward += cur_reward\n",
    "                reward += cur_reward\n",
    "                state_next = torch.Tensor([state_next[-1]]).unsqueeze(0)\n",
    "\n",
    "        reward = torch.tensor([reward]).unsqueeze(0)        \n",
    "        terminal = torch.tensor([int(terminal)]).unsqueeze(0)\n",
    "        \n",
    "        if training_mode:\n",
    "            agent.remember(state, two_actions_index, reward, state_next, terminal)\n",
    "            agent.experience_replay()\n",
    "        \n",
    "        state = state_next\n",
    "        if terminal:\n",
    "            break\n",
    "\n",
    "    total_info.append(info)\n",
    "    total_rewards.append(total_reward)\n",
    "\n",
    "    if training_mode and (ep_num % ep_per_stat) == 0:\n",
    "        save_checkpoint(agent, total_rewards, total_info, run_id)\n",
    "\n",
    "    with open(f'total_reward-{run_id}.txt', 'a') as f:\n",
    "        f.write(\"Total reward after episode {} is {}\\n\".format(ep_num + 1, total_rewards[-1]))\n",
    "        if (ep_num%100 == 0):\n",
    "            f.write(\"==================\\n\")\n",
    "            f.write(\"{} current time at episode {}\\n\".format(datetime.datetime.now(), ep_num+1))\n",
    "            f.write(\"==================\\n\")\n",
    "        #print(\"Total reward after episode {} is {}\".format(ep_num + 1, total_rewards[-1]))\n",
    "        num_episodes += 1\n",
    "\n",
    "if training_mode:\n",
    "    save_checkpoint(agent, total_rewards, run_id)\n",
    "\n",
    "env.close()\n",
    "fh.close()\n",
    "\n",
    "if num_episodes > ep_per_stat:\n",
    "    plt.title(\"Episodes trained vs. Average Rewards (per 500 eps)\")\n",
    "    plt.plot([0 for _ in range(ep_per_stat)] + \n",
    "                np.convolve(total_rewards, np.ones((ep_per_stat,))/ep_per_stat, mode=\"valid\").tolist())\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(SIMPLE_MOVEMENT) + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.cur_action_space.shape\n",
    "rand_ind = random.randrange(0, agent.cur_action_space.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6])\n",
      "torch.Size([1, 10])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "array split does not result in an equal division",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m two_actions_vector \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39mcur_action_space[\u001b[39m0\u001b[39m, two_actions_index]\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(two_actions_vector\u001b[39m.\u001b[39mshape)\n\u001b[0;32m----> 4\u001b[0m two_actions \u001b[39m=\u001b[39m vec_to_action(two_actions_vector\u001b[39m.\u001b[39;49mcpu()) \u001b[39m# tuple of actions\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/CSCI_566/maRLios/toolkit/action_utils.py:42\u001b[0m, in \u001b[0;36mvec_to_action\u001b[0;34m(vec)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvec_to_action\u001b[39m(vec):\n\u001b[1;32m     38\u001b[0m \u001b[39m    \u001b[39m\u001b[39m'''\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[39m    vector is a combination of two actions \u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[39m    sample vector[left, right, down, a, b, |split between actions| left, right, down, a, b]\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m     split_vec \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49msplit(vec, \u001b[39m2\u001b[39;49m)\n\u001b[1;32m     44\u001b[0m     act1_vec \u001b[39m=\u001b[39m split_vec[\u001b[39m0\u001b[39m]\n\u001b[1;32m     45\u001b[0m     act2_vec \u001b[39m=\u001b[39m split_vec[\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36msplit\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/Desktop/CSCI_566/env/lib/python3.10/site-packages/numpy/lib/shape_base.py:872\u001b[0m, in \u001b[0;36msplit\u001b[0;34m(ary, indices_or_sections, axis)\u001b[0m\n\u001b[1;32m    870\u001b[0m     N \u001b[39m=\u001b[39m ary\u001b[39m.\u001b[39mshape[axis]\n\u001b[1;32m    871\u001b[0m     \u001b[39mif\u001b[39;00m N \u001b[39m%\u001b[39m sections:\n\u001b[0;32m--> 872\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    873\u001b[0m             \u001b[39m'\u001b[39m\u001b[39marray split does not result in an equal division\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    874\u001b[0m \u001b[39mreturn\u001b[39;00m array_split(ary, indices_or_sections, axis)\n",
      "\u001b[0;31mValueError\u001b[0m: array split does not result in an equal division"
     ]
    }
   ],
   "source": [
    "print(two_actions_index)\n",
    "two_actions_vector = agent.cur_action_space[0, two_actions_index]\n",
    "print(two_actions_vector.shape)\n",
    "two_actions = vec_to_action(two_actions_vector.cpu()) # tuple of actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0211, -0.0230, -0.0210, -0.0221, -0.0215, -0.0214, -0.0200, -0.0203,\n",
      "         -0.0200]], grad_fn=<ToCopyBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([6])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = agent.local_net(state.to(agent.device), agent.cur_action_space).cpu()\n",
    "print(results)\n",
    "torch.argmax(results, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = env.reset()[-1] # take the final dimension of shape (4, 84, 84) leaving shape (84, 84) \n",
    "x = torch.Tensor([x]).unsqueeze(0).to(agent.device) # converts (1, 84, 84) to (1, 1, 84, 84)\n",
    "# print(state.shape)\n",
    "\n",
    "conv_out = agent.local_net.conv(x).view(x.size()[0], -1)\n",
    "batched_conv_out = conv_out.reshape(conv_out.shape[0], 1, conv_out.shape[-1]).repeat(1, agent.cur_action_space.shape[-2], 1)\n",
    "batched_actions = torch.cat((batched_conv_out, agent.cur_action_space), dim=2)\n",
    "intermediate = agent.local_net.fc(batched_actions)\n",
    "out =  torch.flatten(intermediate, start_dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(intermediate)\n",
    "\n",
    "double = torch.cat((intermediate, intermediate + 1))\n",
    "out = torch.flatten(double, start_dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATE, ACTION, REWARD, STATE2, DONE, SPACE = agent.recall()\n",
    "STATE = STATE.to(agent.device)\n",
    "ACTION = ACTION.to(agent.device)\n",
    "REWARD = REWARD.to(agent.device)\n",
    "STATE2 = STATE2.to(agent.device)\n",
    "SPACE = SPACE.to(agent.device)\n",
    "DONE = DONE.to(agent.device)\n",
    "\n",
    "agent.optimizer.zero_grad()\n",
    "# Double Q-Learning target is Q*(S, A) <- r + γ max_a Q_target(S', a)\n",
    "\n",
    "target = REWARD + torch.mul((agent.gamma * \n",
    "                            agent.target_net(STATE2, SPACE).max(1).values.unsqueeze(1)), \n",
    "                            1 - DONE)\n",
    "\n",
    "current = agent.local_net(STATE, SPACE).gather(1, ACTION.long()) # Local net approximation of Q-value\n",
    "\n",
    "\n",
    "loss = agent.l1(current, target) # maybe we can play with some L2 loss \n",
    "loss.backward() # Compute gradients\n",
    "agent.optimizer.step() # Backpropagate error\n",
    "\n",
    "# agent.cur_action_space = torch.from_numpy(agent.subsample_actions(agent.n_actions)).to(torch.float32).to(agent.device)\n",
    "# I am disabling this here for my testing, but also think we should add it to the run loop for testing til we are sure it works, idk\n",
    "\n",
    "agent.exploration_rate *= agent.exploration_decay\n",
    "\n",
    "# Makes sure that exploration rate is always at least 'exploration min'\n",
    "agent.exploration_rate = max(agent.exploration_rate, agent.exploration_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = STATE\n",
    "sampled_actions = SPACE\n",
    "\n",
    "conv_out = agent.local_net.conv(x).view(x.size()[0], -1)\n",
    "batched_conv_out = conv_out.reshape(conv_out.shape[0], 1, conv_out.shape[-1]).repeat(1, sampled_actions.shape[-2], 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking what the model was doing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_mode=False\n",
    "pretrained=True\n",
    "lr=0.00025\n",
    "gamma=0.90\n",
    "exploration_decay=0.995\n",
    "exploration_min=0.02\n",
    "ep_per_stat = 100\n",
    "mario_env='SuperMarioBros-1-1-v0'\n",
    "action_space=SIMPLE_MOVEMENT\n",
    "num_episodes=1000\n",
    "# run_id='1681699251'\n",
    "n_actions=len(SIMPLE_MOVEMENT) + 2\n",
    "consecutiveActions = 2\n",
    "\n",
    "run_id = run_id or generate_epoch_time_id() \n",
    "fh = open(f'progress-{run_id}.txt', 'a')\n",
    "env = gym.make(mario_env)\n",
    "#env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
    "\n",
    "#env = make_env(env)  # Wraps the environment so that frames are grayscale \n",
    "#env = SuperMarioBrosEnv()\n",
    "env = make_env(env, ACTION_SPACE)\n",
    "# observation_space = env.observation_space.shape # not using this anymore\n",
    "\n",
    "\n",
    "#todo: add agent params as a setting/create different agents in diff functions to run \n",
    "\n",
    "agent = DQNAgent(\n",
    "                    action_space=action_space,\n",
    "                    max_memory_size=30000,\n",
    "                    batch_size=32,\n",
    "                    gamma=gamma,\n",
    "                    lr=lr,\n",
    "                    dropout=0.,\n",
    "                    exploration_max=.02,\n",
    "                    exploration_min=exploration_min,\n",
    "                    exploration_decay=exploration_decay,\n",
    "                    double_dq=True,\n",
    "                    pretrained=pretrained,\n",
    "                    run_id=run_id,\n",
    "                    n_actions=n_actions)\n",
    "\n",
    "\n",
    "# num_episodes = 10\n",
    "env.reset()\n",
    "total_rewards = []\n",
    "total_info = []\n",
    "\n",
    "for ep_num in tqdm(range(num_episodes)):\n",
    "    state = env.reset()[-1] # take the final dimension of shape (4, 84, 84) leaving shape (84, 84) \n",
    "    state = torch.Tensor([state]).unsqueeze(0) # converts (1, 84, 84) to (1, 1, 84, 84)\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    while True:\n",
    "        # if not training_mode:\n",
    "            # show_state(env, ep_num)\n",
    "\n",
    "\n",
    "        two_actions_index = agent.act(state)\n",
    "\n",
    "        two_actions_vector = agent.cur_action_space[0, two_actions_index]\n",
    "\n",
    "        two_actions = vec_to_action(two_actions_vector.cpu()) # tuple of actions\n",
    "        \n",
    "        steps += 1\n",
    "        reward = 0\n",
    "        info = None\n",
    "        terminal = False\n",
    "        for action in two_actions: \n",
    "            if not terminal:\n",
    "                # compute index into ACTION_SPACE of our action\n",
    "                step_action = ACTION_TO_INDEX[action]\n",
    "\n",
    "                state_next, cur_reward, terminal, info = env.step(step_action)\n",
    "                total_reward += cur_reward\n",
    "                reward += cur_reward\n",
    "\n",
    "        state_next = torch.Tensor([state_next[-1]]).unsqueeze(0)\n",
    "        reward = torch.tensor([reward]).unsqueeze(0)        \n",
    "        terminal = torch.tensor([int(terminal)]).unsqueeze(0)\n",
    "        \n",
    "        if training_mode:\n",
    "            agent.remember(state, two_actions_index, reward, state_next, terminal)\n",
    "            agent.experience_replay()\n",
    "        \n",
    "        state = state_next\n",
    "        if terminal:\n",
    "            break\n",
    "\n",
    "    total_info.append(info)\n",
    "    total_rewards.append(total_reward)\n",
    "\n",
    "    if training_mode and (ep_num % ep_per_stat) == 0:\n",
    "        save_checkpoint(agent, total_rewards, total_info, run_id)\n",
    "\n",
    "    with open(f'total_reward-{run_id}.txt', 'a') as f:\n",
    "        f.write(\"Total reward after episode {} is {}\\n\".format(ep_num + 1, total_rewards[-1]))\n",
    "        if (ep_num%100 == 0):\n",
    "            f.write(\"==================\\n\")\n",
    "            f.write(\"{} current time at episode {}\\n\".format(datetime.datetime.now(), ep_num+1))\n",
    "            f.write(\"==================\\n\")\n",
    "        #print(\"Total reward after episode {} is {}\".format(ep_num + 1, total_rewards[-1]))\n",
    "        num_episodes += 1\n",
    "\n",
    "if training_mode:\n",
    "    save_checkpoint(agent, total_rewards, run_id)\n",
    "\n",
    "env.close()\n",
    "fh.close()\n",
    "\n",
    "if num_episodes > ep_per_stat:\n",
    "    plt.title(\"Episodes trained vs. Average Rewards (per 500 eps)\")\n",
    "    plt.plot([0 for _ in range(ep_per_stat)] + \n",
    "                np.convolve(total_rewards, np.ones((ep_per_stat,))/ep_per_stat, mode=\"valid\").tolist())\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(agent.n_actions):\n",
    "    two_actions_vector = agent.cur_action_space[0, i]\n",
    "    two_actions = vec_to_action(two_actions_vector.cpu()) # tuple of actions\n",
    "    print(two_actions)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
