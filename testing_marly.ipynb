{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #only run once\n",
    "# #!pip install nes-py==0.2.6\n",
    "# !brew update\n",
    "# !brew install ffmpeg\n",
    "# !brew install libsm\n",
    "# !brew install libxext\n",
    "# !brew install mesa\n",
    "# !pip install opencv-python\n",
    "# !pip install gym-super-mario-bros\n",
    "# !pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import gym\n",
    "import gym_super_mario_bros\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "from gym_super_mario_bros import SuperMarioBrosEnv\n",
    "from tqdm import tqdm\n",
    "import pickle \n",
    "import gym\n",
    "import numpy as np\n",
    "import collections \n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import datetime\n",
    "import json\n",
    "from toolkit.gym_env import *\n",
    "from toolkit.action_utils import *\n",
    "from toolkit.marlios_model import *\n",
    "from toolkit.constants import *\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_state(env, ep=0, info=\"\"):\n",
    "    plt.figure(3)\n",
    "    plt.clf()\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    plt.title(\"Episode: %d %s\" % (ep, info))\n",
    "    plt.axis('off')\n",
    "\n",
    "    # display.clear_output(wait=True)\n",
    "    # display.display(plt.gcf())\n",
    "    display(plt.gcf(), clear=True)\n",
    "\n",
    "def make_env(env, actions=ACTION_SPACE):\n",
    "    env = MaxAndSkipEnv(env, skip=2) # I am testing out fewer fram repetitions for our two actions modelling\n",
    "    env = ProcessFrame84(env)\n",
    "    env = ImageToPyTorch(env)\n",
    "    env = BufferWrapper(env, 4)\n",
    "    env = ScaledFloatFrame(env)\n",
    "    return JoypadSpace(env, actions)\n",
    "\n",
    "def generate_epoch_time_id():\n",
    "    epoch_time = int(time.time())\n",
    "    return str(epoch_time)\n",
    "\n",
    "def save_checkpoint(agent, total_rewards, terminal_info, run_id):\n",
    "    with open(f\"ending_position-{run_id}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(agent.ending_position, f)\n",
    "    with open(f\"num_in_queue-{run_id}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(agent.num_in_queue, f)\n",
    "    with open(f\"total_rewards-{run_id}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(total_rewards, f)\n",
    "    with open(f\"terminal_info-{run_id}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(terminal_info, f)\n",
    "    if agent.double_dq:\n",
    "        torch.save(agent.local_net.state_dict(), f\"dq1-{run_id}.pt\")\n",
    "        torch.save(agent.target_net.state_dict(), f\"dq2-{run_id}.pt\")\n",
    "    else:\n",
    "        torch.save(agent.dqn.state_dict(), f\"dq-{run_id}.pt\")  \n",
    "\n",
    "def load_rewards(from_file):\n",
    "     with open(from_file, 'rb') as f:\n",
    "        total_rewards = pickle.load(f)\n",
    "        return total_rewards\n",
    "\n",
    "def plot_rewards(ep_per_stat = 100, total_rewards = [], from_file = None):\n",
    "    if from_file != None:\n",
    "        total_rewards = load_rewards(total_rewards)\n",
    "       \n",
    "    avg_rewards = [np.mean(total_rewards[i:i+ep_per_stat]) for i in range(0, len(total_rewards), ep_per_stat)]\n",
    "    std_rewards = [np.std(total_rewards[i:i+ep_per_stat]) for i in range(0, len(total_rewards), ep_per_stat)]\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(avg_rewards, label='Average Rewards')\n",
    "    ax.fill_between(range(len(avg_rewards)), np.subtract(avg_rewards, std_rewards), np.add(avg_rewards, std_rewards), alpha=0.2, label='Reward StdDev')\n",
    "\n",
    "    ax.set_xlabel('Episode')\n",
    "    ax.set_ylabel('Reward')\n",
    "    xtick_labels = [str(i*ep_per_stat) for i in range(len(avg_rewards))]\n",
    "    plt.xticks(range(1, len(avg_rewards)+1), xtick_labels)\n",
    "    plt.xticks(rotation=45)\n",
    "    ax.legend(loc='lower right')\n",
    "    plt.show()\n",
    "\n",
    "# define the run function with some helpful debugging stats\n",
    "\n",
    "def run(training_mode=True, pretrained=False, lr=0.0001, gamma=0.90, exploration_decay=0.995, exploration_min=0.02, \n",
    "        ep_per_stat = 100, exploration_max = 1, lr_decay = 0.999,\n",
    "        mario_env='SuperMarioBros-1-1-v0', action_space=TWO_ACTIONS_SET, num_episodes=1000, run_id=None, n_actions=5, debug = True):\n",
    "   \n",
    "    run_id = run_id or generate_epoch_time_id()\n",
    "    fh = open(f'progress-{run_id}.txt', 'a')\n",
    "    env = gym.make(mario_env)\n",
    "    env = make_env(env, ACTION_SPACE)\n",
    "\n",
    "\n",
    "    # observation_space = env.observation_space.shape # not using this anymore\n",
    "\n",
    "    #todo: add agent params as a setting/create different agents in diff functions to run \n",
    "    exploration_max = min(1, max(exploration_max, exploration_min))\n",
    "\n",
    "    agent = DQNAgent(\n",
    "                     state_space=env.observation_space.shape,\n",
    "                     action_space=action_space,\n",
    "                     max_memory_size=30000,\n",
    "                     batch_size=64,\n",
    "                     gamma=gamma,\n",
    "                     lr=lr,\n",
    "                     dropout=None,\n",
    "                     exploration_max=exploration_max,\n",
    "                     exploration_min=exploration_min,\n",
    "                     exploration_decay=exploration_decay,\n",
    "                     double_dq=True,\n",
    "                     pretrained=pretrained,\n",
    "                     run_id=run_id,\n",
    "                     n_actions=n_actions)\n",
    "    \n",
    "    \n",
    "    # num_episodes = 10\n",
    "    env.reset()\n",
    "    total_rewards = []\n",
    "    total_info = []\n",
    "    total_losses = []\n",
    "    if pretrained:\n",
    "        total_rewards = load_rewards(from_file='total_rewards-{}.pkl'.format(run_id))\n",
    "    \n",
    "    offset = len(total_rewards)   \n",
    "    for iteration in tqdm(range(num_episodes)):\n",
    "        ep_num = offset + iteration\n",
    "\n",
    "        state = env.reset() # take the final dimension of shape \n",
    "        state = torch.Tensor([state])# converts (1, 84, 84) to (1, 1, 84, 84)\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "\n",
    "        action_freq = {}\n",
    "        while True:\n",
    "            if not training_mode:\n",
    "                show_state(env, ep_num)\n",
    "\n",
    "\n",
    "            two_actions_index = agent.act(state)\n",
    "            two_actions_vector = agent.cur_action_space[0, two_actions_index[0]]\n",
    "            two_actions = vec_to_action(two_actions_vector.cpu()) # tuple of actions\n",
    "\n",
    "            # debugging info\n",
    "            key = \" | \".join([\",\".join(i) for i in two_actions])\n",
    "            if key in action_freq:\n",
    "                action_freq[key] += 1\n",
    "            else:\n",
    "                action_freq[key] = 1\n",
    "            \n",
    "            steps += 1\n",
    "            reward = 0\n",
    "            info = None\n",
    "            terminal = False\n",
    "            for action in two_actions: \n",
    "                if not terminal:\n",
    "                    # compute index into ACTION_SPACE of our action\n",
    "                    step_action = ACTION_TO_INDEX[action]\n",
    "\n",
    "                    state_next, cur_reward, terminal, info = env.step(step_action)\n",
    "                    total_reward += cur_reward\n",
    "                    reward += cur_reward\n",
    "                    \n",
    "            state_next = torch.Tensor([state_next])\n",
    "            reward = torch.tensor([reward]).unsqueeze(0)        \n",
    "            terminal = torch.tensor([int(terminal)]).unsqueeze(0)\n",
    "            \n",
    "            if training_mode:\n",
    "                agent.remember(state, two_actions_index, reward, state_next, terminal)\n",
    "                loss = agent.experience_replay(debug=debug)\n",
    "                total_losses.append(loss)\n",
    "            \n",
    "            state = state_next\n",
    "            if terminal:\n",
    "                break\n",
    "\n",
    "        total_info.append(info)\n",
    "        total_rewards.append(total_reward)\n",
    "\n",
    "        # decay LR\n",
    "        # agent.decay_exploration()\n",
    "        agent.decay_lr(lr_decay)\n",
    "        \n",
    "        if training_mode and (ep_num % ep_per_stat) == 0 and ep_num != 0:\n",
    "            save_checkpoint(agent, total_rewards, total_info, run_id)\n",
    "\n",
    "        with open(f'total_reward-{run_id}.txt', 'a') as f:\n",
    "            f.write(\"Total reward after episode {} is {}\\n\".format(ep_num + 1, total_rewards[-1]))\n",
    "            if (ep_num%100 == 0):\n",
    "                f.write(\"==================\\n\")\n",
    "                f.write(\"{} current time at episode {}\\n\".format(datetime.datetime.now(), ep_num+1))\n",
    "                f.write(\"==================\\n\")\n",
    "            #print(\"Total reward after episode {} is {}\".format(ep_num + 1, total_rewards[-1]))\n",
    "            num_episodes += 1\n",
    "        \n",
    "        with open(f'actions_chosen-{run_id}.txt', 'a') as f:\n",
    "            f.write(\"Action Frequencies for Episode {}, Exploration = {:4f}\\n\".format(ep_num + 1, agent.exploration_rate))\n",
    "            f.write(json.dumps(action_freq) + \"\\n\\n\")\n",
    "        \n",
    "        if debug:\n",
    "            with open(f'loss-{run_id}.txt', 'a') as f:\n",
    "                f.write(\"loss: {}\".format(loss))\n",
    "    \n",
    "    if training_mode:\n",
    "        save_checkpoint(agent, total_rewards, total_info, run_id)\n",
    "    \n",
    "    env.close()\n",
    "    fh.close()\n",
    "    \n",
    "    if num_episodes > ep_per_stat:\n",
    "        plot_rewards(ep_per_stat=ep_per_stat, total_rewards=total_rewards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/6000 [00:04<7:16:04,  4.36s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# rerunning fresh\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m run(training_mode\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, \n\u001b[1;32m      4\u001b[0m     pretrained\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, \u001b[39m# use the pretrained model\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m     ep_per_stat\u001b[39m=\u001b[39;49m\u001b[39m200\u001b[39;49m, \n\u001b[1;32m      6\u001b[0m     gamma\u001b[39m=\u001b[39;49m\u001b[39m0.7\u001b[39;49m,\n\u001b[1;32m      7\u001b[0m     num_episodes\u001b[39m=\u001b[39;49m\u001b[39m6000\u001b[39;49m,\n\u001b[1;32m      8\u001b[0m     run_id\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m      9\u001b[0m     lr\u001b[39m=\u001b[39;49m\u001b[39m0.00025\u001b[39;49m,\n\u001b[1;32m     10\u001b[0m     lr_decay\u001b[39m=\u001b[39;49m \u001b[39m0.999\u001b[39;49m,\n\u001b[1;32m     11\u001b[0m     exploration_min\u001b[39m=\u001b[39;49m\u001b[39m0.02\u001b[39;49m,\n\u001b[1;32m     12\u001b[0m     exploration_max \u001b[39m=\u001b[39;49m \u001b[39m1\u001b[39;49m, \u001b[39m# setting this to the min for the rerun model\u001b[39;49;00m\n\u001b[1;32m     13\u001b[0m     exploration_decay\u001b[39m=\u001b[39;49m\u001b[39m0.995\u001b[39;49m, \n\u001b[1;32m     14\u001b[0m     action_space\u001b[39m=\u001b[39;49mSIMPLE_MOVEMENT,\n\u001b[1;32m     15\u001b[0m     n_actions\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(SIMPLE_MOVEMENT),\n\u001b[1;32m     16\u001b[0m     debug\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[1;32m     17\u001b[0m     )\n\u001b[1;32m     20\u001b[0m \u001b[39m# #rerunning pretrained\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[39m# run_id = '1682047848'\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[39m# run(training_mode=True, \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39m#     n_actions=len(SIMPLE_MOVEMENT)\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[39m#     )\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 150\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(training_mode, pretrained, lr, gamma, exploration_decay, exploration_min, ep_per_stat, exploration_max, lr_decay, mario_env, action_space, num_episodes, run_id, n_actions, debug)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[39mif\u001b[39;00m training_mode:\n\u001b[1;32m    149\u001b[0m     agent\u001b[39m.\u001b[39mremember(state, two_actions_index, reward, state_next, terminal)\n\u001b[0;32m--> 150\u001b[0m     loss \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49mexperience_replay(debug\u001b[39m=\u001b[39;49mdebug)\n\u001b[1;32m    151\u001b[0m     total_losses\u001b[39m.\u001b[39mappend(loss)\n\u001b[1;32m    153\u001b[0m state \u001b[39m=\u001b[39m state_next\n",
      "File \u001b[0;32m~/Desktop/CSCI_566/maRLios/toolkit/marlios_model.py:304\u001b[0m, in \u001b[0;36mDQNAgent.experience_replay\u001b[0;34m(self, debug)\u001b[0m\n\u001b[1;32m    300\u001b[0m current \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlocal_net(STATE, SPACE)\u001b[39m.\u001b[39mgather(\u001b[39m1\u001b[39m, ACTION\u001b[39m.\u001b[39mlong()) \u001b[39m# Local net approximation of Q-value\u001b[39;00m\n\u001b[1;32m    303\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39ml1(current, target) \u001b[39m# maybe we can play with some L2 loss \u001b[39;00m\n\u001b[0;32m--> 304\u001b[0m loss\u001b[39m.\u001b[39;49mbackward() \u001b[39m# Compute gradients\u001b[39;00m\n\u001b[1;32m    305\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep() \u001b[39m# Backpropagate error\u001b[39;00m\n\u001b[1;32m    307\u001b[0m \u001b[39m# self.cur_action_space = torch.from_numpy(self.subsample_actions(self.n_actions)).to(torch.float32).to(self.device)\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[39m# I am disabling this here for my testing, but also think we should add it to the run loop for testing til we are sure it works, idk\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[39m# decay lr\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/CSCI_566/env/lib/python3.10/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    490\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/CSCI_566/env/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# rerunning fresh\n",
    "\n",
    "run(training_mode=True, \n",
    "    pretrained=False, # use the pretrained model\n",
    "    ep_per_stat=200, \n",
    "    gamma=0.7,\n",
    "    num_episodes=6000,\n",
    "    run_id=None,\n",
    "    lr=0.00025,\n",
    "    lr_decay= 0.999,\n",
    "    exploration_min=0.02,\n",
    "    exploration_max = 1, # setting this to the min for the rerun model\n",
    "    exploration_decay=0.995, \n",
    "    action_space=SIMPLE_MOVEMENT,\n",
    "    n_actions=len(SIMPLE_MOVEMENT),\n",
    "    debug=True\n",
    "    )\n",
    "\n",
    "\n",
    "# #rerunning pretrained\n",
    "# run_id = '1682047848'\n",
    "# run(training_mode=True, \n",
    "#     pretrained=True, # use the pretrained model\n",
    "#     ep_per_stat=100, \n",
    "#     num_episodes=1000,\n",
    "#     run_id=run_id,\n",
    "#     lr=0.0001,\n",
    "#     exploration_min=0.02,\n",
    "#     exploration_max = 0.5, # setting this to the min for the rerun model\n",
    "#     exploration_decay=0.995, \n",
    "#     action_space=SIMPLE_MOVEMENT,\n",
    "#     n_actions=len(SIMPLE_MOVEMENT)\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(run_id, action_space, n_actions, lr=0.0001, exploration_min=0.02, ep_per_stat = 100, exploration_max = 0.1, mario_env='SuperMarioBros-1-1-v0',  num_episodes=1000, log_stats = False):\n",
    "   \n",
    "   \n",
    "    fh = open(f'progress-{run_id}.txt', 'a')\n",
    "    env = gym.make(mario_env)\n",
    "    env = make_env(env, ACTION_SPACE)\n",
    "\n",
    "\n",
    "    # observation_space = env.observation_space.shape # not using this anymore\n",
    "\n",
    "    #todo: add agent params as a setting/create different agents in diff functions to run \n",
    "    exploration_max = min(1, max(exploration_max, exploration_min))\n",
    "\n",
    "    agent = DQNAgent(\n",
    "                     state_space=env.observation_space.shape,\n",
    "                     action_space=action_space,\n",
    "                     max_memory_size=30000,\n",
    "                     batch_size=64,\n",
    "                     gamma=0.9,\n",
    "                     lr=lr,\n",
    "                     dropout=None,\n",
    "                     exploration_max=exploration_max,\n",
    "                     exploration_min=exploration_min,\n",
    "                     exploration_decay=0.9995,\n",
    "                     double_dq=True,\n",
    "                     pretrained=True,\n",
    "                     run_id=run_id,\n",
    "                     n_actions=n_actions)\n",
    "    \n",
    "    \n",
    "    # num_episodes = 10\n",
    "    env.reset()\n",
    "    total_rewards = []\n",
    "    total_info = []\n",
    " \n",
    "    for ep_num in tqdm(range(num_episodes)):\n",
    "      \n",
    "        state = env.reset() # take the final dimension of shape \n",
    "        state = torch.Tensor([state])# converts (1, 84, 84) to (1, 1, 84, 84)\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "\n",
    "        action_freq = {}\n",
    "        while True:\n",
    "\n",
    "            show_state(env, ep_num)\n",
    "\n",
    "            two_actions_index = agent.act(state)\n",
    "            two_actions_vector = agent.cur_action_space[0, two_actions_index[0]]\n",
    "            two_actions = vec_to_action(two_actions_vector.cpu()) # tuple of actions\n",
    "            \n",
    "            print(two_actions)\n",
    "\n",
    "            # debugging info\n",
    "            key = \" | \".join([\",\".join(i) for i in two_actions])\n",
    "            if key in action_freq:\n",
    "                action_freq[key] += 1\n",
    "            else:\n",
    "                action_freq[key] = 1\n",
    "            \n",
    "            steps += 1\n",
    "            reward = 0\n",
    "            info = None\n",
    "            terminal = False\n",
    "            for action in two_actions: \n",
    "                if not terminal:\n",
    "                    # compute index into ACTION_SPACE of our action\n",
    "                    step_action = ACTION_TO_INDEX[action]\n",
    "\n",
    "                    state_next, cur_reward, terminal, info = env.step(step_action)\n",
    "                    total_reward += cur_reward\n",
    "                    reward += cur_reward\n",
    "                    \n",
    "            state_next = torch.Tensor([state_next])\n",
    "            reward = torch.tensor([reward]).unsqueeze(0)        \n",
    "            terminal = torch.tensor([int(terminal)]).unsqueeze(0)\n",
    "            \n",
    "            \n",
    "            state = state_next\n",
    "            if terminal:\n",
    "                break\n",
    "\n",
    "        total_info.append(info)\n",
    "        total_rewards.append(total_reward)\n",
    "\n",
    "        if log_stats:\n",
    "            with open(f'visualized_rewards-{run_id}.txt', 'a') as f:\n",
    "                f.write(\"Total reward after episode {} is {}\\n\".format(ep_num + 1, total_rewards[-1]))\n",
    "                if (ep_num%100 == 0):\n",
    "                    f.write(\"==================\\n\")\n",
    "                    f.write(\"{} current time at episode {}\\n\".format(datetime.datetime.now(), ep_num+1))\n",
    "                    f.write(\"==================\\n\")\n",
    "                #print(\"Total reward after episode {} is {}\".format(ep_num + 1, total_rewards[-1]))\n",
    "                num_episodes += 1\n",
    "            \n",
    "            with open(f'visualized_actions_chosen-{run_id}.txt', 'a') as f:\n",
    "                f.write(\"Action Frequencies for Episode {}, Exploration = {:4f}\\n\".format(ep_num + 1, agent.exploration_rate))\n",
    "                f.write(json.dumps(action_freq) + \"\\n\\n\")\n",
    "        \n",
    "    \n",
    "    env.close()\n",
    "    fh.close()\n",
    "    \n",
    "    if num_episodes > ep_per_stat:\n",
    "        plot_rewards(ep_per_stat=ep_per_stat, total_rewards=total_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = '1682062196' # left left guy\n",
    "visualize(run_id=run_id, action_space=SIMPLE_MOVEMENT, n_actions=len(SIMPLE_MOVEMENT), ep_per_stat=200, num_episodes=100, exploration_max=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing this specific run\n",
    "action_space=SIMPLE_MOVEMENT\n",
    "action_space.append([['A', 'B', 'down', 'right'], ['A']])\n",
    "\n",
    "run_id = '1681968442'\n",
    "# setting it to sample all the dif actions \n",
    "visualize(run_id=run_id, action_space=action_space, n_actions=len(action_space)+1, ep_per_stat=100, num_episodes=500, exploration_max=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space=SIMPLE_MOVEMENT\n",
    "action_space.append([['A', 'B', 'down', 'right'], ['A']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
