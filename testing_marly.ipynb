{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #only run once\n",
    "# #!pip install nes-py==0.2.6\n",
    "# !brew update\n",
    "# !brew install ffmpeg\n",
    "# !brew install libsm\n",
    "# !brew install libxext\n",
    "# !brew install mesa\n",
    "# !pip install opencv-python\n",
    "# !pip install gym-super-mario-bros\n",
    "# !pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import gym\n",
    "import gym_super_mario_bros\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "from gym_super_mario_bros import SuperMarioBrosEnv\n",
    "from tqdm import tqdm\n",
    "import pickle \n",
    "import gym\n",
    "import numpy as np\n",
    "import collections \n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import datetime\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from toolkit.gym_env import *\n",
    "from toolkit.action_utils import *\n",
    "from toolkit.marlios_model import *\n",
    "from toolkit.constants import *\n",
    "\n",
    "CONSECUTIVE_ACTIONS = 2\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_state(env, ep=0, info=\"\"):\n",
    "    plt.figure(3)\n",
    "    plt.clf()\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    plt.title(\"Episode: %d %s\" % (ep, info))\n",
    "    plt.axis('off')\n",
    "\n",
    "    # display.clear_output(wait=True)\n",
    "    # display.display(plt.gcf())\n",
    "    display(plt.gcf(), clear=True)\n",
    "\n",
    "def make_env(env, actions=ACTION_SPACE):\n",
    "    env = MaxAndSkipEnv(env, skip=4) # I am testing out fewer fram repetitions for our two actions modelling\n",
    "    env = ProcessFrame84(env)\n",
    "    env = ImageToPyTorch(env)\n",
    "    env = BufferWrapper(env, 4)\n",
    "    env = ScaledFloatFrame(env)\n",
    "    return JoypadSpace(env, actions)\n",
    "\n",
    "def generate_epoch_time_id():\n",
    "    epoch_time = int(time.time())\n",
    "    return str(epoch_time)\n",
    "\n",
    "def save_checkpoint(agent, total_rewards, terminal_info, run_id):\n",
    "    with open(f\"ending_position-{run_id}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(agent.ending_position, f)\n",
    "    with open(f\"num_in_queue-{run_id}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(agent.num_in_queue, f)\n",
    "    with open(f\"total_rewards-{run_id}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(total_rewards, f)\n",
    "    with open(f\"terminal_info-{run_id}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(terminal_info, f)\n",
    "    if agent.double_dq:\n",
    "        torch.save(agent.local_net.state_dict(), f\"dq1-{run_id}.pt\")\n",
    "        torch.save(agent.target_net.state_dict(), f\"dq2-{run_id}.pt\")\n",
    "    else:\n",
    "        torch.save(agent.dqn.state_dict(), f\"dq-{run_id}.pt\")  \n",
    "\n",
    "def load_rewards(from_file):\n",
    "     with open(from_file, 'rb') as f:\n",
    "        total_rewards = pickle.load(f)\n",
    "        return total_rewards\n",
    "\n",
    "def plot_rewards(ep_per_stat = 100, total_rewards = [], from_file = None):\n",
    "    if from_file != None:\n",
    "        total_rewards = load_rewards(total_rewards)\n",
    "       \n",
    "    avg_rewards = [np.mean(total_rewards[i:i+ep_per_stat]) for i in range(0, len(total_rewards), ep_per_stat)]\n",
    "    std_rewards = [np.std(total_rewards[i:i+ep_per_stat]) for i in range(0, len(total_rewards), ep_per_stat)]\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(avg_rewards, label='Average Rewards')\n",
    "    ax.fill_between(range(len(avg_rewards)), np.subtract(avg_rewards, std_rewards), np.add(avg_rewards, std_rewards), alpha=0.2, label='Reward StdDev')\n",
    "\n",
    "    ax.set_xlabel('Episode')\n",
    "    ax.set_ylabel('Reward')\n",
    "    xtick_labels = [str(i*ep_per_stat) for i in range(len(avg_rewards))]\n",
    "    plt.xticks(range(len(avg_rewards)), xtick_labels)\n",
    "    ax.legend(loc='lower right')\n",
    "    plt.show()\n",
    "\n",
    "# define the run function with some helpful debugging stats\n",
    "\n",
    "def run(training_mode=True, pretrained=False, lr=0.0001, gamma=0.90, exploration_decay=0.995, exploration_min=0.02, \n",
    "        ep_per_stat = 100, exploration_max = 1,\n",
    "        mario_env='SuperMarioBros-1-1-v0', action_space=TWO_ACTIONS_SET, num_episodes=1000, run_id=None, n_actions=5):\n",
    "   \n",
    "    run_id = run_id or generate_epoch_time_id()\n",
    "    fh = open(f'progress-{run_id}.txt', 'a')\n",
    "    env = gym.make(mario_env)\n",
    "    env = make_env(env, ACTION_SPACE)\n",
    "\n",
    "\n",
    "    # observation_space = env.observation_space.shape # not using this anymore\n",
    "\n",
    "    #todo: add agent params as a setting/create different agents in diff functions to run \n",
    "    exploration_max = min(1, max(exploration_max, exploration_min))\n",
    "\n",
    "    agent = DQNAgent(\n",
    "                     action_space=action_space,\n",
    "                     max_memory_size=30000,\n",
    "                     batch_size=64,\n",
    "                     gamma=gamma,\n",
    "                     lr=lr,\n",
    "                     dropout=None,\n",
    "                     exploration_max=exploration_max,\n",
    "                     exploration_min=exploration_min,\n",
    "                     exploration_decay=exploration_decay,\n",
    "                     double_dq=True,\n",
    "                     pretrained=pretrained,\n",
    "                     run_id=run_id,\n",
    "                     n_actions=n_actions)\n",
    "    \n",
    "    \n",
    "    # num_episodes = 10\n",
    "    env.reset()\n",
    "    total_rewards = []\n",
    "    total_info = []\n",
    "    \n",
    "    for ep_num in tqdm(range(num_episodes)):\n",
    "        state = env.reset()[-1] # take the final dimension of shape (4, 84, 84) leaving shape (84, 84) \n",
    "        state = torch.Tensor([state]).unsqueeze(0) # converts (1, 84, 84) to (1, 1, 84, 84)\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "\n",
    "        action_freq = {}\n",
    "        while True:\n",
    "            if not training_mode:\n",
    "                show_state(env, ep_num)\n",
    "\n",
    "\n",
    "            two_actions_index = agent.act(state)\n",
    "            two_actions_vector = agent.cur_action_space[0, two_actions_index[0]]\n",
    "            two_actions = vec_to_action(two_actions_vector.cpu()) # tuple of actions\n",
    "\n",
    "            # debugging info\n",
    "            key = \" | \".join([\",\".join(i) for i in two_actions])\n",
    "            if key in action_freq:\n",
    "                action_freq[key] += 1\n",
    "            else:\n",
    "                action_freq[key] = 1\n",
    "            \n",
    "            steps += 1\n",
    "            reward = 0\n",
    "            info = None\n",
    "            terminal = False\n",
    "            for action in two_actions: \n",
    "                if not terminal:\n",
    "                    # compute index into ACTION_SPACE of our action\n",
    "                    step_action = ACTION_TO_INDEX[action]\n",
    "\n",
    "                    state_next, cur_reward, terminal, info = env.step(step_action)\n",
    "                    total_reward += cur_reward\n",
    "                    reward += cur_reward\n",
    "                    \n",
    "            state_next = torch.Tensor([state_next[-1]]).unsqueeze(0)\n",
    "            reward = torch.tensor([reward]).unsqueeze(0)        \n",
    "            terminal = torch.tensor([int(terminal)]).unsqueeze(0)\n",
    "            \n",
    "            if training_mode:\n",
    "                agent.remember(state, two_actions_index, reward, state_next, terminal)\n",
    "                agent.experience_replay()\n",
    "            \n",
    "            state = state_next\n",
    "            if terminal:\n",
    "                break\n",
    "\n",
    "        total_info.append(info)\n",
    "        total_rewards.append(total_reward)\n",
    "\n",
    "        if training_mode and (ep_num % ep_per_stat) == 0:\n",
    "            save_checkpoint(agent, total_rewards, total_info, run_id)\n",
    "\n",
    "        with open(f'total_reward-{run_id}.txt', 'a') as f:\n",
    "            f.write(\"Total reward after episode {} is {}\\n\".format(ep_num + 1, total_rewards[-1]))\n",
    "            if (ep_num%100 == 0):\n",
    "                f.write(\"==================\\n\")\n",
    "                f.write(\"{} current time at episode {}\\n\".format(datetime.datetime.now(), ep_num+1))\n",
    "                f.write(\"==================\\n\")\n",
    "            #print(\"Total reward after episode {} is {}\".format(ep_num + 1, total_rewards[-1]))\n",
    "            num_episodes += 1\n",
    "        \n",
    "        with open(f'actions_chosen-{run_id}.txt', 'a') as f:\n",
    "            f.write(\"Action Frequencies for Episode {}, Exploration = {:4f}\\n\".format(ep_num + 1, agent.exploration_rate))\n",
    "            f.write(json.dumps(action_freq) + \"\\n\\n\")\n",
    "    \n",
    "    if training_mode:\n",
    "        save_checkpoint(agent, total_rewards, total_info, run_id)\n",
    "    \n",
    "    env.close()\n",
    "    fh.close()\n",
    "    \n",
    "    if num_episodes > ep_per_stat:\n",
    "        plot_rewards(ep_per_stat=ep_per_stat, total_rewards=total_rewards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings. filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 920/1000 [45:31<16:59, 12.74s/it]  "
     ]
    }
   ],
   "source": [
    "# rerun with pretrained model:\n",
    "run(training_mode=True, \n",
    "    pretrained=False, # use the pretrained model\n",
    "    ep_per_stat=100, \n",
    "    num_episodes=1000,\n",
    "    # run_id=run_id,\n",
    "    lr=0.00001,\n",
    "    exploration_min=0.02,\n",
    "    exploration_max = 1,\n",
    "    exploration_decay=0.9995, \n",
    "    action_space=SIMPLE_MOVEMENT,\n",
    "    n_actions=len(SIMPLE_MOVEMENT)+2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"total_rewards-{}.pkl\".format(run_id)\n",
    "total_rewards = load_rewards(file_name)\n",
    "\n",
    "plot_rewards(ep_per_stat=100, total_rewards=total_rewards)\n",
    "\n",
    "# ep_per_stat = 100  # number of episodes to average over"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_mode=True\n",
    "pretrained=False\n",
    "lr=0.000005\n",
    "gamma=0.90\n",
    "exploration_decay=0.995\n",
    "exploration_min=0.02\n",
    "ep_per_stat = 100\n",
    "mario_env='SuperMarioBros-1-1-v0'\n",
    "action_space=SIMPLE_MOVEMENT\n",
    "num_episodes=1000\n",
    "run_id=None\n",
    "n_actions=len(SIMPLE_MOVEMENT) + 2\n",
    "consecutiveActions = 2\n",
    "\n",
    "run_id = run_id or generate_epoch_time_id()\n",
    "fh = open(f'progress-{run_id}.txt', 'a')\n",
    "env = gym.make(mario_env)\n",
    "#env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
    "\n",
    "#env = make_env(env)  # Wraps the environment so that frames are grayscale \n",
    "#env = SuperMarioBrosEnv()\n",
    "env = make_env(env, ACTION_SPACE)\n",
    "# observation_space = env.observation_space.shape # not using this anymore\n",
    "\n",
    "#todo: add agent params as a setting/create different agents in diff functions to run \n",
    "\n",
    "agent = DQNAgent(\n",
    "                    action_space=action_space,\n",
    "                    max_memory_size=30000,\n",
    "                    batch_size=64,\n",
    "                    gamma=gamma,\n",
    "                    lr=lr,\n",
    "                    dropout=0.,\n",
    "                    exploration_max=1,\n",
    "                    exploration_min=exploration_min,\n",
    "                    exploration_decay=exploration_decay,\n",
    "                    double_dq=True,\n",
    "                    pretrained=pretrained,\n",
    "                    run_id=run_id,\n",
    "                    n_actions=n_actions)\n",
    "\n",
    "\n",
    "# num_episodes = 10\n",
    "env.reset()\n",
    "total_rewards = []\n",
    "total_info = []\n",
    "\n",
    "for ep_num in tqdm(range(num_episodes)):\n",
    "    state = env.reset()[-1] # take the final dimension of shape (4, 84, 84) leaving shape (84, 84) \n",
    "    state = torch.Tensor([state]).unsqueeze(0) # converts (1, 84, 84) to (1, 1, 84, 84)\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    while True:\n",
    "        if not training_mode:\n",
    "            show_state(env, ep_num)\n",
    "\n",
    "\n",
    "        two_actions_index = agent.act(state)\n",
    "        # print()\n",
    "        # print(two_actions_index.float())\n",
    "        two_actions_vector = agent.cur_action_space[0, two_actions_index[0]]\n",
    "        # print(two_actions_vector)\n",
    "\n",
    "        two_actions = vec_to_action(two_actions_vector.cpu()) # tuple of actions\n",
    "\n",
    "        if ep_num % 5 == 0 and ep_num != 0:\n",
    "            print(two_actions)\n",
    "\n",
    "        steps += 1\n",
    "        reward = 0\n",
    "        info = None\n",
    "        terminal = False\n",
    "        for action in two_actions: \n",
    "            if not terminal:\n",
    "                # compute index into ACTION_SPACE of our action\n",
    "                step_action = ACTION_TO_INDEX[action]\n",
    "\n",
    "                state_next, cur_reward, terminal, info = env.step(step_action)\n",
    "                total_reward += cur_reward\n",
    "                reward += cur_reward\n",
    "                state_next = torch.Tensor([state_next[-1]]).unsqueeze(0)\n",
    "\n",
    "        reward = torch.tensor([reward]).unsqueeze(0)        \n",
    "        terminal = torch.tensor([int(terminal)]).unsqueeze(0)\n",
    "        \n",
    "        if training_mode:\n",
    "            agent.remember(state, two_actions_index, reward, state_next, terminal)\n",
    "            agent.experience_replay()\n",
    "        \n",
    "        state = state_next\n",
    "        if terminal:\n",
    "            break\n",
    "\n",
    "    total_info.append(info)\n",
    "    total_rewards.append(total_reward)\n",
    "\n",
    "    if training_mode and (ep_num % ep_per_stat) == 0:\n",
    "        save_checkpoint(agent, total_rewards, total_info, run_id)\n",
    "\n",
    "    with open(f'total_reward-{run_id}.txt', 'a') as f:\n",
    "        f.write(\"Total reward after episode {} is {}\\n\".format(ep_num + 1, total_rewards[-1]))\n",
    "        if (ep_num%100 == 0):\n",
    "            f.write(\"==================\\n\")\n",
    "            f.write(\"{} current time at episode {}\\n\".format(datetime.datetime.now(), ep_num+1))\n",
    "            f.write(\"==================\\n\")\n",
    "        #print(\"Total reward after episode {} is {}\".format(ep_num + 1, total_rewards[-1]))\n",
    "        num_episodes += 1\n",
    "\n",
    "if training_mode:\n",
    "    save_checkpoint(agent, total_rewards, run_id)\n",
    "\n",
    "env.close()\n",
    "fh.close()\n",
    "\n",
    "if num_episodes > ep_per_stat:\n",
    "    plt.title(\"Episodes trained vs. Average Rewards (per 500 eps)\")\n",
    "    plt.plot([0 for _ in range(ep_per_stat)] + \n",
    "                np.convolve(total_rewards, np.ones((ep_per_stat,))/ep_per_stat, mode=\"valid\").tolist())\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(SIMPLE_MOVEMENT) + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.cur_action_space.shape\n",
    "rand_ind = random.randrange(0, agent.cur_action_space.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(two_actions_index)\n",
    "two_actions_vector = agent.cur_action_space[0, two_actions_index]\n",
    "print(two_actions_vector.shape)\n",
    "two_actions = vec_to_action(two_actions_vector.cpu()) # tuple of actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = agent.local_net(state.to(agent.device), agent.cur_action_space).cpu()\n",
    "print(results)\n",
    "torch.argmax(results, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = env.reset()[-1] # take the final dimension of shape (4, 84, 84) leaving shape (84, 84) \n",
    "x = torch.Tensor([x]).unsqueeze(0).to(agent.device) # converts (1, 84, 84) to (1, 1, 84, 84)\n",
    "# print(state.shape)\n",
    "\n",
    "conv_out = agent.local_net.conv(x).view(x.size()[0], -1)\n",
    "batched_conv_out = conv_out.reshape(conv_out.shape[0], 1, conv_out.shape[-1]).repeat(1, agent.cur_action_space.shape[-2], 1)\n",
    "batched_actions = torch.cat((batched_conv_out, agent.cur_action_space), dim=2)\n",
    "intermediate = agent.local_net.fc(batched_actions)\n",
    "out =  torch.flatten(intermediate, start_dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(intermediate)\n",
    "\n",
    "double = torch.cat((intermediate, intermediate + 1))\n",
    "out = torch.flatten(double, start_dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATE, ACTION, REWARD, STATE2, DONE, SPACE = agent.recall()\n",
    "STATE = STATE.to(agent.device)\n",
    "ACTION = ACTION.to(agent.device)\n",
    "REWARD = REWARD.to(agent.device)\n",
    "STATE2 = STATE2.to(agent.device)\n",
    "SPACE = SPACE.to(agent.device)\n",
    "DONE = DONE.to(agent.device)\n",
    "\n",
    "agent.optimizer.zero_grad()\n",
    "# Double Q-Learning target is Q*(S, A) <- r + γ max_a Q_target(S', a)\n",
    "\n",
    "target = REWARD + torch.mul((agent.gamma * \n",
    "                            agent.target_net(STATE2, SPACE).max(1).values.unsqueeze(1)), \n",
    "                            1 - DONE)\n",
    "\n",
    "current = agent.local_net(STATE, SPACE).gather(1, ACTION.long()) # Local net approximation of Q-value\n",
    "\n",
    "\n",
    "loss = agent.l1(current, target) # maybe we can play with some L2 loss \n",
    "loss.backward() # Compute gradients\n",
    "agent.optimizer.step() # Backpropagate error\n",
    "\n",
    "# agent.cur_action_space = torch.from_numpy(agent.subsample_actions(agent.n_actions)).to(torch.float32).to(agent.device)\n",
    "# I am disabling this here for my testing, but also think we should add it to the run loop for testing til we are sure it works, idk\n",
    "\n",
    "agent.exploration_rate *= agent.exploration_decay\n",
    "\n",
    "# Makes sure that exploration rate is always at least 'exploration min'\n",
    "agent.exploration_rate = max(agent.exploration_rate, agent.exploration_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = STATE\n",
    "sampled_actions = SPACE\n",
    "\n",
    "conv_out = agent.local_net.conv(x).view(x.size()[0], -1)\n",
    "batched_conv_out = conv_out.reshape(conv_out.shape[0], 1, conv_out.shape[-1]).repeat(1, sampled_actions.shape[-2], 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking what the model was doing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_mode=False\n",
    "pretrained=True\n",
    "lr=0.00025\n",
    "gamma=0.90\n",
    "exploration_decay=0.995\n",
    "exploration_min=0.02\n",
    "ep_per_stat = 100\n",
    "mario_env='SuperMarioBros-1-1-v0'\n",
    "action_space=SIMPLE_MOVEMENT\n",
    "num_episodes=1000\n",
    "# run_id='1681699251'\n",
    "n_actions=len(SIMPLE_MOVEMENT) + 2\n",
    "consecutiveActions = 2\n",
    "\n",
    "run_id = run_id or generate_epoch_time_id() \n",
    "fh = open(f'progress-{run_id}.txt', 'a')\n",
    "env = gym.make(mario_env)\n",
    "#env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
    "\n",
    "#env = make_env(env)  # Wraps the environment so that frames are grayscale \n",
    "#env = SuperMarioBrosEnv()\n",
    "env = make_env(env, ACTION_SPACE)\n",
    "# observation_space = env.observation_space.shape # not using this anymore\n",
    "\n",
    "\n",
    "#todo: add agent params as a setting/create different agents in diff functions to run \n",
    "\n",
    "agent = DQNAgent(\n",
    "                    action_space=action_space,\n",
    "                    max_memory_size=30000,\n",
    "                    batch_size=32,\n",
    "                    gamma=gamma,\n",
    "                    lr=lr,\n",
    "                    dropout=0.,\n",
    "                    exploration_max=.02,\n",
    "                    exploration_min=exploration_min,\n",
    "                    exploration_decay=exploration_decay,\n",
    "                    double_dq=True,\n",
    "                    pretrained=pretrained,\n",
    "                    run_id=run_id,\n",
    "                    n_actions=n_actions)\n",
    "\n",
    "\n",
    "# num_episodes = 10\n",
    "env.reset()\n",
    "total_rewards = []\n",
    "total_info = []\n",
    "\n",
    "for ep_num in tqdm(range(num_episodes)):\n",
    "    state = env.reset()[-1] # take the final dimension of shape (4, 84, 84) leaving shape (84, 84) \n",
    "    state = torch.Tensor([state]).unsqueeze(0) # converts (1, 84, 84) to (1, 1, 84, 84)\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    while True:\n",
    "        # if not training_mode:\n",
    "            # show_state(env, ep_num)\n",
    "\n",
    "\n",
    "        two_actions_index = agent.act(state)\n",
    "\n",
    "        two_actions_vector = agent.cur_action_space[0, two_actions_index]\n",
    "\n",
    "        two_actions = vec_to_action(two_actions_vector.cpu()) # tuple of actions\n",
    "        \n",
    "        steps += 1\n",
    "        reward = 0\n",
    "        info = None\n",
    "        terminal = False\n",
    "        for action in two_actions: \n",
    "            if not terminal:\n",
    "                # compute index into ACTION_SPACE of our action\n",
    "                step_action = ACTION_TO_INDEX[action]\n",
    "\n",
    "                state_next, cur_reward, terminal, info = env.step(step_action)\n",
    "                total_reward += cur_reward\n",
    "                reward += cur_reward\n",
    "\n",
    "        state_next = torch.Tensor([state_next[-1]]).unsqueeze(0)\n",
    "        reward = torch.tensor([reward]).unsqueeze(0)        \n",
    "        terminal = torch.tensor([int(terminal)]).unsqueeze(0)\n",
    "        \n",
    "        if training_mode:\n",
    "            agent.remember(state, two_actions_index, reward, state_next, terminal)\n",
    "            agent.experience_replay()\n",
    "        \n",
    "        state = state_next\n",
    "        if terminal:\n",
    "            break\n",
    "\n",
    "    total_info.append(info)\n",
    "    total_rewards.append(total_reward)\n",
    "\n",
    "    if training_mode and (ep_num % ep_per_stat) == 0:\n",
    "        save_checkpoint(agent, total_rewards, total_info, run_id)\n",
    "\n",
    "    with open(f'total_reward-{run_id}.txt', 'a') as f:\n",
    "        f.write(\"Total reward after episode {} is {}\\n\".format(ep_num + 1, total_rewards[-1]))\n",
    "        if (ep_num%100 == 0):\n",
    "            f.write(\"==================\\n\")\n",
    "            f.write(\"{} current time at episode {}\\n\".format(datetime.datetime.now(), ep_num+1))\n",
    "            f.write(\"==================\\n\")\n",
    "        #print(\"Total reward after episode {} is {}\".format(ep_num + 1, total_rewards[-1]))\n",
    "        num_episodes += 1\n",
    "\n",
    "if training_mode:\n",
    "    save_checkpoint(agent, total_rewards, run_id)\n",
    "\n",
    "env.close()\n",
    "fh.close()\n",
    "\n",
    "if num_episodes > ep_per_stat:\n",
    "    plt.title(\"Episodes trained vs. Average Rewards (per 500 eps)\")\n",
    "    plt.plot([0 for _ in range(ep_per_stat)] + \n",
    "                np.convolve(total_rewards, np.ones((ep_per_stat,))/ep_per_stat, mode=\"valid\").tolist())\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(agent.n_actions):\n",
    "    two_actions_vector = agent.cur_action_space[0, i]\n",
    "    two_actions = vec_to_action(two_actions_vector.cpu()) # tuple of actions\n",
    "    print(two_actions)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
