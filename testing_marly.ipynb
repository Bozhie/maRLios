{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #only run once\n",
    "# #!pip install nes-py==0.2.6\n",
    "# !brew update\n",
    "# !brew install ffmpeg\n",
    "# !brew install libsm\n",
    "# !brew install libxext\n",
    "# !brew install mesa\n",
    "# !pip install opencv-python\n",
    "# !pip install gym-super-mario-bros\n",
    "# !pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import gym\n",
    "import gym_super_mario_bros\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "from gym_super_mario_bros import SuperMarioBrosEnv\n",
    "from tqdm import tqdm\n",
    "import pickle \n",
    "import gym\n",
    "import numpy as np\n",
    "import collections \n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import datetime\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from toolkit.gym_env import *\n",
    "from toolkit.action_utils import *\n",
    "from toolkit.marlios_model import *\n",
    "from toolkit.constants import *\n",
    "\n",
    "CONSECUTIVE_ACTIONS = 2\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_state(env, ep=0, info=\"\"):\n",
    "    plt.figure(3)\n",
    "    plt.clf()\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    plt.title(\"Episode: %d %s\" % (ep, info))\n",
    "    plt.axis('off')\n",
    "\n",
    "    # display.clear_output(wait=True)\n",
    "    # display.display(plt.gcf())\n",
    "    display(plt.gcf(), clear=True)\n",
    "\n",
    "def make_env(env, actions=ACTION_SPACE):\n",
    "    env = MaxAndSkipEnv(env, skip=2) # I am testing out fewer fram repetitions for our two actions modelling\n",
    "    env = ProcessFrame84(env)\n",
    "    env = ImageToPyTorch(env)\n",
    "    env = BufferWrapper(env, 4)\n",
    "    env = ScaledFloatFrame(env)\n",
    "    return JoypadSpace(env, actions)\n",
    "\n",
    "def generate_epoch_time_id():\n",
    "    epoch_time = int(time.time())\n",
    "    return str(epoch_time)\n",
    "\n",
    "def save_checkpoint(agent, total_rewards, terminal_info, run_id):\n",
    "    with open(f\"ending_position-{run_id}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(agent.ending_position, f)\n",
    "    with open(f\"num_in_queue-{run_id}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(agent.num_in_queue, f)\n",
    "    with open(f\"total_rewards-{run_id}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(total_rewards, f)\n",
    "    with open(f\"terminal_info-{run_id}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(terminal_info, f)\n",
    "    if agent.double_dq:\n",
    "        torch.save(agent.local_net.state_dict(), f\"dq1-{run_id}.pt\")\n",
    "        torch.save(agent.target_net.state_dict(), f\"dq2-{run_id}.pt\")\n",
    "    else:\n",
    "        torch.save(agent.dqn.state_dict(), f\"dq-{run_id}.pt\")  \n",
    "\n",
    "def load_rewards(from_file):\n",
    "     with open(from_file, 'rb') as f:\n",
    "        total_rewards = pickle.load(f)\n",
    "        return total_rewards\n",
    "\n",
    "def plot_rewards(ep_per_stat = 100, total_rewards = [], from_file = None):\n",
    "    if from_file != None:\n",
    "        total_rewards = load_rewards(total_rewards)\n",
    "       \n",
    "    avg_rewards = [np.mean(total_rewards[i:i+ep_per_stat]) for i in range(0, len(total_rewards), ep_per_stat)]\n",
    "    std_rewards = [np.std(total_rewards[i:i+ep_per_stat]) for i in range(0, len(total_rewards), ep_per_stat)]\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(avg_rewards, label='Average Rewards')\n",
    "    ax.fill_between(range(len(avg_rewards)), np.subtract(avg_rewards, std_rewards), np.add(avg_rewards, std_rewards), alpha=0.2, label='Reward StdDev')\n",
    "\n",
    "    ax.set_xlabel('Episode')\n",
    "    ax.set_ylabel('Reward')\n",
    "    xtick_labels = [str(i*ep_per_stat) for i in range(len(avg_rewards))]\n",
    "    plt.xticks(range(len(avg_rewards)), xtick_labels)\n",
    "    ax.legend(loc='lower right')\n",
    "    plt.show()\n",
    "\n",
    "# define the run function with some helpful debugging stats\n",
    "\n",
    "def run(training_mode=True, pretrained=False, lr=0.0001, gamma=0.90, exploration_decay=0.995, exploration_min=0.02, \n",
    "        ep_per_stat = 100, exploration_max = 1,\n",
    "        mario_env='SuperMarioBros-1-1-v0', action_space=TWO_ACTIONS_SET, num_episodes=1000, run_id=None, n_actions=5):\n",
    "   \n",
    "    run_id = run_id or generate_epoch_time_id()\n",
    "    fh = open(f'progress-{run_id}.txt', 'a')\n",
    "    env = gym.make(mario_env)\n",
    "    env = make_env(env, ACTION_SPACE)\n",
    "\n",
    "\n",
    "    # observation_space = env.observation_space.shape # not using this anymore\n",
    "\n",
    "    #todo: add agent params as a setting/create different agents in diff functions to run \n",
    "    exploration_max = min(1, max(exploration_max, exploration_min))\n",
    "\n",
    "    agent = DQNAgent(\n",
    "                     action_space=action_space,\n",
    "                     max_memory_size=30000,\n",
    "                     batch_size=32,\n",
    "                     gamma=gamma,\n",
    "                     lr=lr,\n",
    "                     dropout=0.,\n",
    "                     exploration_max=exploration_max,\n",
    "                     exploration_min=exploration_min,\n",
    "                     exploration_decay=exploration_decay,\n",
    "                     double_dq=True,\n",
    "                     pretrained=pretrained,\n",
    "                     run_id=run_id,\n",
    "                     n_actions=n_actions)\n",
    "    \n",
    "    \n",
    "    # num_episodes = 10\n",
    "    env.reset()\n",
    "    total_rewards = []\n",
    "    total_info = []\n",
    "    \n",
    "    for ep_num in tqdm(range(num_episodes)):\n",
    "        state = env.reset()[-1] # take the final dimension of shape (4, 84, 84) leaving shape (84, 84) \n",
    "        state = torch.Tensor([state]).unsqueeze(0) # converts (1, 84, 84) to (1, 1, 84, 84)\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "\n",
    "        action_freq = {}\n",
    "        while True:\n",
    "            if not training_mode:\n",
    "                show_state(env, ep_num)\n",
    "\n",
    "\n",
    "            two_actions_index = agent.act(state)\n",
    "            two_actions_vector = agent.cur_action_space[0, two_actions_index[0]]\n",
    "            two_actions = vec_to_action(two_actions_vector.cpu()) # tuple of actions\n",
    "\n",
    "            # debugging info\n",
    "            key = \" | \".join([\",\".join(i) for i in two_actions])\n",
    "            if key in action_freq:\n",
    "                action_freq[key] += 1\n",
    "            else:\n",
    "                action_freq[key] = 1\n",
    "            \n",
    "            steps += 1\n",
    "            reward = 0\n",
    "            info = None\n",
    "            terminal = False\n",
    "            for action in two_actions: \n",
    "                if not terminal:\n",
    "                    # compute index into ACTION_SPACE of our action\n",
    "                    step_action = ACTION_TO_INDEX[action]\n",
    "\n",
    "                    state_next, cur_reward, terminal, info = env.step(step_action)\n",
    "                    total_reward += cur_reward\n",
    "                    reward += cur_reward\n",
    "                    \n",
    "            state_next = torch.Tensor([state_next[-1]]).unsqueeze(0)\n",
    "            reward = torch.tensor([reward]).unsqueeze(0)        \n",
    "            terminal = torch.tensor([int(terminal)]).unsqueeze(0)\n",
    "            \n",
    "            if training_mode:\n",
    "                agent.remember(state, two_actions_index, reward, state_next, terminal)\n",
    "                agent.experience_replay()\n",
    "            \n",
    "            state = state_next\n",
    "            if terminal:\n",
    "                break\n",
    "\n",
    "        total_info.append(info)\n",
    "        total_rewards.append(total_reward)\n",
    "\n",
    "        if training_mode and (ep_num % ep_per_stat) == 0:\n",
    "            save_checkpoint(agent, total_rewards, total_info, run_id)\n",
    "\n",
    "        with open(f'total_reward-{run_id}.txt', 'a') as f:\n",
    "            f.write(\"Total reward after episode {} is {}\\n\".format(ep_num + 1, total_rewards[-1]))\n",
    "            if (ep_num%100 == 0):\n",
    "                f.write(\"==================\\n\")\n",
    "                f.write(\"{} current time at episode {}\\n\".format(datetime.datetime.now(), ep_num+1))\n",
    "                f.write(\"==================\\n\")\n",
    "            #print(\"Total reward after episode {} is {}\".format(ep_num + 1, total_rewards[-1]))\n",
    "            num_episodes += 1\n",
    "        \n",
    "        with open(f'actions_chosen-{run_id}.txt', 'a') as f:\n",
    "            f.write(\"Action Frequencies for Episode {}, Exploration = {:4f}\\n\".format(ep_num + 1, agent.exploration_rate))\n",
    "            f.write(json.dumps(action_freq) + \"\\n\\n\")\n",
    "    \n",
    "    if training_mode:\n",
    "        save_checkpoint(agent, total_rewards, total_info, run_id)\n",
    "    \n",
    "    env.close()\n",
    "    fh.close()\n",
    "    \n",
    "    if num_episodes > ep_per_stat:\n",
    "        plot_rewards(ep_per_stat=ep_per_stat, total_rewards=total_rewards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = '1681801872'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cameronwitz/Desktop/CSCI_566/env/lib/python3.10/site-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment SuperMarioBros-1-1-v0 is out of date. You should consider upgrading to version `v3`.\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/cameronwitz/Desktop/CSCI_566/env/lib/python3.10/site-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "/Users/cameronwitz/Desktop/CSCI_566/env/lib/python3.10/site-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "  0%|          | 0/1000 [00:00<?, ?it/s]/Users/cameronwitz/Desktop/CSCI_566/env/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
      "  logger.deprecation(\n",
      "/Users/cameronwitz/Desktop/CSCI_566/env/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(done, (bool, np.bool8)):\n",
      " 70%|███████   | 701/1000 [2:02:20<52:10, 10.47s/it]    \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[107], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# rerun with pretrained model:\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m run(training_mode\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, \n\u001b[1;32m      3\u001b[0m     pretrained\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, \u001b[39m# use the pretrained model\u001b[39;49;00m\n\u001b[1;32m      4\u001b[0m     ep_per_stat\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, \n\u001b[1;32m      5\u001b[0m     run_id\u001b[39m=\u001b[39;49mrun_id,\n\u001b[1;32m      6\u001b[0m     lr\u001b[39m=\u001b[39;49m\u001b[39m0.000005\u001b[39;49m,\n\u001b[1;32m      7\u001b[0m     exploration_min\u001b[39m=\u001b[39;49m\u001b[39m0.1\u001b[39;49m,\n\u001b[1;32m      8\u001b[0m     exploration_max \u001b[39m=\u001b[39;49m \u001b[39m0.1\u001b[39;49m,\n\u001b[1;32m      9\u001b[0m     exploration_decay\u001b[39m=\u001b[39;49m\u001b[39m0.9995\u001b[39;49m, \n\u001b[1;32m     10\u001b[0m     action_space\u001b[39m=\u001b[39;49mSIMPLE_MOVEMENT,\n\u001b[1;32m     11\u001b[0m     n_actions\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(SIMPLE_MOVEMENT)\u001b[39m+\u001b[39;49m\u001b[39m2\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[101], line 120\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(training_mode, pretrained, lr, gamma, exploration_decay, exploration_min, ep_per_stat, exploration_max, mario_env, action_space, num_episodes, run_id, n_actions)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[39mif\u001b[39;00m training_mode:\n\u001b[1;32m    119\u001b[0m     agent\u001b[39m.\u001b[39mremember(state, two_actions_index, reward, state_next, terminal)\n\u001b[0;32m--> 120\u001b[0m     agent\u001b[39m.\u001b[39;49mexperience_replay()\n\u001b[1;32m    122\u001b[0m state \u001b[39m=\u001b[39m state_next\n\u001b[1;32m    123\u001b[0m \u001b[39mif\u001b[39;00m terminal:\n",
      "File \u001b[0;32m~/Desktop/CSCI_566/maRLios/toolkit/marlios_model.py:219\u001b[0m, in \u001b[0;36mDQNAgent.experience_replay\u001b[0;34m(self, debug)\u001b[0m\n\u001b[1;32m    215\u001b[0m current \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlocal_net(STATE, SPACE)\u001b[39m.\u001b[39mgather(\u001b[39m1\u001b[39m, ACTION\u001b[39m.\u001b[39mlong()) \u001b[39m# Local net approximation of Q-value\u001b[39;00m\n\u001b[1;32m    218\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39ml1(current, target) \u001b[39m# maybe we can play with some L2 loss \u001b[39;00m\n\u001b[0;32m--> 219\u001b[0m loss\u001b[39m.\u001b[39;49mbackward() \u001b[39m# Compute gradients\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep() \u001b[39m# Backpropagate error\u001b[39;00m\n\u001b[1;32m    222\u001b[0m \u001b[39m# self.cur_action_space = torch.from_numpy(self.subsample_actions(self.n_actions)).to(torch.float32).to(self.device)\u001b[39;00m\n\u001b[1;32m    223\u001b[0m \u001b[39m# I am disabling this here for my testing, but also think we should add it to the run loop for testing til we are sure it works, idk\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/CSCI_566/env/lib/python3.10/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    490\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/CSCI_566/env/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# rerun with pretrained model:\n",
    "run(training_mode=True, \n",
    "    pretrained=True, # use the pretrained model\n",
    "    ep_per_stat=100, \n",
    "    run_id=run_id,\n",
    "    lr=0.000005,\n",
    "    exploration_min=0.1,\n",
    "    exploration_max = 0.1,\n",
    "    exploration_decay=0.9995, \n",
    "    action_space=SIMPLE_MOVEMENT,\n",
    "    n_actions=len(SIMPLE_MOVEMENT)+2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "file_name = \"total_rewards-{}.pkl\".format(run_id)\n",
    "total_rewards = load_rewards(file_name)\n",
    "\n",
    "plot_rewards(ep_per_stat=100, total_rewards=total_rewards)\n",
    "\n",
    "# ep_per_stat = 100  # number of episodes to average over"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/1000 [04:08<17:09:22, 62.01s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[74], line 78\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m terminal:\n\u001b[1;32m     75\u001b[0m     \u001b[39m# compute index into ACTION_SPACE of our action\u001b[39;00m\n\u001b[1;32m     76\u001b[0m     step_action \u001b[39m=\u001b[39m ACTION_TO_INDEX[action]\n\u001b[0;32m---> 78\u001b[0m     state_next, cur_reward, terminal, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(step_action)\n\u001b[1;32m     79\u001b[0m     total_reward \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m cur_reward\n\u001b[1;32m     80\u001b[0m     reward \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m cur_reward\n",
      "File \u001b[0;32m~/Desktop/CSCI_566/env/lib/python3.10/site-packages/nes_py/wrappers/joypad_space.py:74\u001b[0m, in \u001b[0;36mJoypadSpace.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[39mTake a step using the given action.\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     71\u001b[0m \n\u001b[1;32m     72\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[39m# take the step and record the output\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_action_map[action])\n",
      "File \u001b[0;32m~/Desktop/CSCI_566/env/lib/python3.10/site-packages/gym/core.py:483\u001b[0m, in \u001b[0;36mObservationWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[1;32m    482\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Returns a modified observation using :meth:`self.observation` after calling :meth:`env.step`.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 483\u001b[0m     step_returns \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m    484\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(step_returns) \u001b[39m==\u001b[39m \u001b[39m5\u001b[39m:\n\u001b[1;32m    485\u001b[0m         observation, reward, terminated, truncated, info \u001b[39m=\u001b[39m step_returns\n",
      "File \u001b[0;32m~/Desktop/CSCI_566/env/lib/python3.10/site-packages/gym/core.py:483\u001b[0m, in \u001b[0;36mObservationWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[1;32m    482\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Returns a modified observation using :meth:`self.observation` after calling :meth:`env.step`.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 483\u001b[0m     step_returns \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m    484\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(step_returns) \u001b[39m==\u001b[39m \u001b[39m5\u001b[39m:\n\u001b[1;32m    485\u001b[0m         observation, reward, terminated, truncated, info \u001b[39m=\u001b[39m step_returns\n",
      "    \u001b[0;31m[... skipping similar frames: ObservationWrapper.step at line 483 (1 times)]\u001b[0m\n",
      "File \u001b[0;32m~/Desktop/CSCI_566/env/lib/python3.10/site-packages/gym/core.py:483\u001b[0m, in \u001b[0;36mObservationWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[1;32m    482\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Returns a modified observation using :meth:`self.observation` after calling :meth:`env.step`.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 483\u001b[0m     step_returns \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m    484\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(step_returns) \u001b[39m==\u001b[39m \u001b[39m5\u001b[39m:\n\u001b[1;32m    485\u001b[0m         observation, reward, terminated, truncated, info \u001b[39m=\u001b[39m step_returns\n",
      "File \u001b[0;32m~/Desktop/CSCI_566/maRLios/toolkit/gym_env.py:27\u001b[0m, in \u001b[0;36mMaxAndSkipEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     25\u001b[0m done \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_skip):\n\u001b[0;32m---> 27\u001b[0m     obs, reward, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     28\u001b[0m     \u001b[39m# obs, reward, done, truncation, info = self.env.step(action)\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_obs_buffer\u001b[39m.\u001b[39mappend(obs)\n",
      "File \u001b[0;32m~/Desktop/CSCI_566/env/lib/python3.10/site-packages/gym/wrappers/time_limit.py:60\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[1;32m     49\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \n\u001b[1;32m     51\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[39m        \"TimeLimit.truncated\"=False if the environment terminated\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m     59\u001b[0m     observation, reward, terminated, truncated, info \u001b[39m=\u001b[39m step_api_compatibility(\n\u001b[0;32m---> 60\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action),\n\u001b[1;32m     61\u001b[0m         \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     62\u001b[0m     )\n\u001b[1;32m     63\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/Desktop/CSCI_566/env/lib/python3.10/site-packages/gym/wrappers/order_enforcing.py:37\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset:\n\u001b[1;32m     36\u001b[0m     \u001b[39mraise\u001b[39;00m ResetNeeded(\u001b[39m\"\u001b[39m\u001b[39mCannot call env.step() before calling env.reset()\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[0;32m~/Desktop/CSCI_566/env/lib/python3.10/site-packages/gym/wrappers/step_api_compatibility.py:52\u001b[0m, in \u001b[0;36mStepAPICompatibility.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[1;32m     44\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Steps through the environment, returning 5 or 4 items depending on `new_step_api`.\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \n\u001b[1;32m     46\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39m        (observation, reward, terminated, truncated, info) or (observation, reward, done, info)\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m     step_returns \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     53\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnew_step_api:\n\u001b[1;32m     54\u001b[0m         \u001b[39mreturn\u001b[39;00m step_to_new_api(step_returns)\n",
      "File \u001b[0;32m~/Desktop/CSCI_566/env/lib/python3.10/site-packages/gym/wrappers/env_checker.py:39\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[39mreturn\u001b[39;00m env_step_passive_checker(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv, action)\n\u001b[1;32m     38\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 39\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[0;32m~/Desktop/CSCI_566/env/lib/python3.10/site-packages/nes_py/nes_env.py:306\u001b[0m, in \u001b[0;36mNESEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdone \u001b[39m=\u001b[39m \u001b[39mbool\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_done())\n\u001b[1;32m    305\u001b[0m \u001b[39m# get the info for this step\u001b[39;00m\n\u001b[0;32m--> 306\u001b[0m info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_info()\n\u001b[1;32m    307\u001b[0m \u001b[39m# call the after step callback\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_did_step(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdone)\n",
      "File \u001b[0;32m~/Desktop/CSCI_566/env/lib/python3.10/site-packages/gym_super_mario_bros/smb_env.py:416\u001b[0m, in \u001b[0;36mSuperMarioBrosEnv._get_info\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_info\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    406\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return the info after a step occurs\"\"\"\u001b[39;00m\n\u001b[1;32m    407\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mdict\u001b[39m(\n\u001b[1;32m    408\u001b[0m         coins\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_coins,\n\u001b[1;32m    409\u001b[0m         flag_get\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flag_get,\n\u001b[1;32m    410\u001b[0m         life\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_life,\n\u001b[1;32m    411\u001b[0m         score\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_score,\n\u001b[1;32m    412\u001b[0m         stage\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stage,\n\u001b[1;32m    413\u001b[0m         status\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_player_status,\n\u001b[1;32m    414\u001b[0m         time\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time,\n\u001b[1;32m    415\u001b[0m         world\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_world,\n\u001b[0;32m--> 416\u001b[0m         x_pos\u001b[39m=\u001b[39m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_x_position,\n\u001b[1;32m    417\u001b[0m         y_pos\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_y_position,\n\u001b[1;32m    418\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/CSCI_566/env/lib/python3.10/site-packages/gym_super_mario_bros/smb_env.py:138\u001b[0m, in \u001b[0;36mSuperMarioBrosEnv._x_position\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return the number of remaining lives.\"\"\"\u001b[39;00m\n\u001b[1;32m    136\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mram[\u001b[39m0x075a\u001b[39m]\n\u001b[0;32m--> 138\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[1;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_x_position\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    140\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return the current horizontal position.\"\"\"\u001b[39;00m\n\u001b[1;32m    141\u001b[0m     \u001b[39m# add the current page 0x6d to the current x\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training_mode=True\n",
    "pretrained=False\n",
    "lr=0.000005\n",
    "gamma=0.90\n",
    "exploration_decay=0.995\n",
    "exploration_min=0.02\n",
    "ep_per_stat = 100\n",
    "mario_env='SuperMarioBros-1-1-v0'\n",
    "action_space=SIMPLE_MOVEMENT\n",
    "num_episodes=1000\n",
    "run_id=None\n",
    "n_actions=len(SIMPLE_MOVEMENT) + 2\n",
    "consecutiveActions = 2\n",
    "\n",
    "run_id = run_id or generate_epoch_time_id()\n",
    "fh = open(f'progress-{run_id}.txt', 'a')\n",
    "env = gym.make(mario_env)\n",
    "#env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
    "\n",
    "#env = make_env(env)  # Wraps the environment so that frames are grayscale \n",
    "#env = SuperMarioBrosEnv()\n",
    "env = make_env(env, ACTION_SPACE)\n",
    "# observation_space = env.observation_space.shape # not using this anymore\n",
    "\n",
    "#todo: add agent params as a setting/create different agents in diff functions to run \n",
    "\n",
    "agent = DQNAgent(\n",
    "                    action_space=action_space,\n",
    "                    max_memory_size=30000,\n",
    "                    batch_size=64,\n",
    "                    gamma=gamma,\n",
    "                    lr=lr,\n",
    "                    dropout=0.,\n",
    "                    exploration_max=1,\n",
    "                    exploration_min=exploration_min,\n",
    "                    exploration_decay=exploration_decay,\n",
    "                    double_dq=True,\n",
    "                    pretrained=pretrained,\n",
    "                    run_id=run_id,\n",
    "                    n_actions=n_actions)\n",
    "\n",
    "\n",
    "# num_episodes = 10\n",
    "env.reset()\n",
    "total_rewards = []\n",
    "total_info = []\n",
    "\n",
    "for ep_num in tqdm(range(num_episodes)):\n",
    "    state = env.reset()[-1] # take the final dimension of shape (4, 84, 84) leaving shape (84, 84) \n",
    "    state = torch.Tensor([state]).unsqueeze(0) # converts (1, 84, 84) to (1, 1, 84, 84)\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    while True:\n",
    "        if not training_mode:\n",
    "            show_state(env, ep_num)\n",
    "\n",
    "\n",
    "        two_actions_index = agent.act(state)\n",
    "        # print()\n",
    "        # print(two_actions_index.float())\n",
    "        two_actions_vector = agent.cur_action_space[0, two_actions_index[0]]\n",
    "        # print(two_actions_vector)\n",
    "\n",
    "        two_actions = vec_to_action(two_actions_vector.cpu()) # tuple of actions\n",
    "\n",
    "        if ep_num % 5 == 0 and ep_num != 0:\n",
    "            print(two_actions)\n",
    "\n",
    "        steps += 1\n",
    "        reward = 0\n",
    "        info = None\n",
    "        terminal = False\n",
    "        for action in two_actions: \n",
    "            if not terminal:\n",
    "                # compute index into ACTION_SPACE of our action\n",
    "                step_action = ACTION_TO_INDEX[action]\n",
    "\n",
    "                state_next, cur_reward, terminal, info = env.step(step_action)\n",
    "                total_reward += cur_reward\n",
    "                reward += cur_reward\n",
    "                state_next = torch.Tensor([state_next[-1]]).unsqueeze(0)\n",
    "\n",
    "        reward = torch.tensor([reward]).unsqueeze(0)        \n",
    "        terminal = torch.tensor([int(terminal)]).unsqueeze(0)\n",
    "        \n",
    "        if training_mode:\n",
    "            agent.remember(state, two_actions_index, reward, state_next, terminal)\n",
    "            agent.experience_replay()\n",
    "        \n",
    "        state = state_next\n",
    "        if terminal:\n",
    "            break\n",
    "\n",
    "    total_info.append(info)\n",
    "    total_rewards.append(total_reward)\n",
    "\n",
    "    if training_mode and (ep_num % ep_per_stat) == 0:\n",
    "        save_checkpoint(agent, total_rewards, total_info, run_id)\n",
    "\n",
    "    with open(f'total_reward-{run_id}.txt', 'a') as f:\n",
    "        f.write(\"Total reward after episode {} is {}\\n\".format(ep_num + 1, total_rewards[-1]))\n",
    "        if (ep_num%100 == 0):\n",
    "            f.write(\"==================\\n\")\n",
    "            f.write(\"{} current time at episode {}\\n\".format(datetime.datetime.now(), ep_num+1))\n",
    "            f.write(\"==================\\n\")\n",
    "        #print(\"Total reward after episode {} is {}\".format(ep_num + 1, total_rewards[-1]))\n",
    "        num_episodes += 1\n",
    "\n",
    "if training_mode:\n",
    "    save_checkpoint(agent, total_rewards, run_id)\n",
    "\n",
    "env.close()\n",
    "fh.close()\n",
    "\n",
    "if num_episodes > ep_per_stat:\n",
    "    plt.title(\"Episodes trained vs. Average Rewards (per 500 eps)\")\n",
    "    plt.plot([0 for _ in range(ep_per_stat)] + \n",
    "                np.convolve(total_rewards, np.ones((ep_per_stat,))/ep_per_stat, mode=\"valid\").tolist())\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(SIMPLE_MOVEMENT) + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.cur_action_space.shape\n",
    "rand_ind = random.randrange(0, agent.cur_action_space.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6])\n",
      "torch.Size([1, 10])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "array split does not result in an equal division",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m two_actions_vector \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39mcur_action_space[\u001b[39m0\u001b[39m, two_actions_index]\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(two_actions_vector\u001b[39m.\u001b[39mshape)\n\u001b[0;32m----> 4\u001b[0m two_actions \u001b[39m=\u001b[39m vec_to_action(two_actions_vector\u001b[39m.\u001b[39;49mcpu()) \u001b[39m# tuple of actions\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/CSCI_566/maRLios/toolkit/action_utils.py:42\u001b[0m, in \u001b[0;36mvec_to_action\u001b[0;34m(vec)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvec_to_action\u001b[39m(vec):\n\u001b[1;32m     38\u001b[0m \u001b[39m    \u001b[39m\u001b[39m'''\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[39m    vector is a combination of two actions \u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[39m    sample vector[left, right, down, a, b, |split between actions| left, right, down, a, b]\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m     split_vec \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49msplit(vec, \u001b[39m2\u001b[39;49m)\n\u001b[1;32m     44\u001b[0m     act1_vec \u001b[39m=\u001b[39m split_vec[\u001b[39m0\u001b[39m]\n\u001b[1;32m     45\u001b[0m     act2_vec \u001b[39m=\u001b[39m split_vec[\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36msplit\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/Desktop/CSCI_566/env/lib/python3.10/site-packages/numpy/lib/shape_base.py:872\u001b[0m, in \u001b[0;36msplit\u001b[0;34m(ary, indices_or_sections, axis)\u001b[0m\n\u001b[1;32m    870\u001b[0m     N \u001b[39m=\u001b[39m ary\u001b[39m.\u001b[39mshape[axis]\n\u001b[1;32m    871\u001b[0m     \u001b[39mif\u001b[39;00m N \u001b[39m%\u001b[39m sections:\n\u001b[0;32m--> 872\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    873\u001b[0m             \u001b[39m'\u001b[39m\u001b[39marray split does not result in an equal division\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    874\u001b[0m \u001b[39mreturn\u001b[39;00m array_split(ary, indices_or_sections, axis)\n",
      "\u001b[0;31mValueError\u001b[0m: array split does not result in an equal division"
     ]
    }
   ],
   "source": [
    "print(two_actions_index)\n",
    "two_actions_vector = agent.cur_action_space[0, two_actions_index]\n",
    "print(two_actions_vector.shape)\n",
    "two_actions = vec_to_action(two_actions_vector.cpu()) # tuple of actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0211, -0.0230, -0.0210, -0.0221, -0.0215, -0.0214, -0.0200, -0.0203,\n",
      "         -0.0200]], grad_fn=<ToCopyBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([6])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = agent.local_net(state.to(agent.device), agent.cur_action_space).cpu()\n",
    "print(results)\n",
    "torch.argmax(results, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = env.reset()[-1] # take the final dimension of shape (4, 84, 84) leaving shape (84, 84) \n",
    "x = torch.Tensor([x]).unsqueeze(0).to(agent.device) # converts (1, 84, 84) to (1, 1, 84, 84)\n",
    "# print(state.shape)\n",
    "\n",
    "conv_out = agent.local_net.conv(x).view(x.size()[0], -1)\n",
    "batched_conv_out = conv_out.reshape(conv_out.shape[0], 1, conv_out.shape[-1]).repeat(1, agent.cur_action_space.shape[-2], 1)\n",
    "batched_actions = torch.cat((batched_conv_out, agent.cur_action_space), dim=2)\n",
    "intermediate = agent.local_net.fc(batched_actions)\n",
    "out =  torch.flatten(intermediate, start_dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(intermediate)\n",
    "\n",
    "double = torch.cat((intermediate, intermediate + 1))\n",
    "out = torch.flatten(double, start_dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATE, ACTION, REWARD, STATE2, DONE, SPACE = agent.recall()\n",
    "STATE = STATE.to(agent.device)\n",
    "ACTION = ACTION.to(agent.device)\n",
    "REWARD = REWARD.to(agent.device)\n",
    "STATE2 = STATE2.to(agent.device)\n",
    "SPACE = SPACE.to(agent.device)\n",
    "DONE = DONE.to(agent.device)\n",
    "\n",
    "agent.optimizer.zero_grad()\n",
    "# Double Q-Learning target is Q*(S, A) <- r + γ max_a Q_target(S', a)\n",
    "\n",
    "target = REWARD + torch.mul((agent.gamma * \n",
    "                            agent.target_net(STATE2, SPACE).max(1).values.unsqueeze(1)), \n",
    "                            1 - DONE)\n",
    "\n",
    "current = agent.local_net(STATE, SPACE).gather(1, ACTION.long()) # Local net approximation of Q-value\n",
    "\n",
    "\n",
    "loss = agent.l1(current, target) # maybe we can play with some L2 loss \n",
    "loss.backward() # Compute gradients\n",
    "agent.optimizer.step() # Backpropagate error\n",
    "\n",
    "# agent.cur_action_space = torch.from_numpy(agent.subsample_actions(agent.n_actions)).to(torch.float32).to(agent.device)\n",
    "# I am disabling this here for my testing, but also think we should add it to the run loop for testing til we are sure it works, idk\n",
    "\n",
    "agent.exploration_rate *= agent.exploration_decay\n",
    "\n",
    "# Makes sure that exploration rate is always at least 'exploration min'\n",
    "agent.exploration_rate = max(agent.exploration_rate, agent.exploration_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = STATE\n",
    "sampled_actions = SPACE\n",
    "\n",
    "conv_out = agent.local_net.conv(x).view(x.size()[0], -1)\n",
    "batched_conv_out = conv_out.reshape(conv_out.shape[0], 1, conv_out.shape[-1]).repeat(1, sampled_actions.shape[-2], 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking what the model was doing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_mode=False\n",
    "pretrained=True\n",
    "lr=0.00025\n",
    "gamma=0.90\n",
    "exploration_decay=0.995\n",
    "exploration_min=0.02\n",
    "ep_per_stat = 100\n",
    "mario_env='SuperMarioBros-1-1-v0'\n",
    "action_space=SIMPLE_MOVEMENT\n",
    "num_episodes=1000\n",
    "# run_id='1681699251'\n",
    "n_actions=len(SIMPLE_MOVEMENT) + 2\n",
    "consecutiveActions = 2\n",
    "\n",
    "run_id = run_id or generate_epoch_time_id() \n",
    "fh = open(f'progress-{run_id}.txt', 'a')\n",
    "env = gym.make(mario_env)\n",
    "#env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
    "\n",
    "#env = make_env(env)  # Wraps the environment so that frames are grayscale \n",
    "#env = SuperMarioBrosEnv()\n",
    "env = make_env(env, ACTION_SPACE)\n",
    "# observation_space = env.observation_space.shape # not using this anymore\n",
    "\n",
    "\n",
    "#todo: add agent params as a setting/create different agents in diff functions to run \n",
    "\n",
    "agent = DQNAgent(\n",
    "                    action_space=action_space,\n",
    "                    max_memory_size=30000,\n",
    "                    batch_size=32,\n",
    "                    gamma=gamma,\n",
    "                    lr=lr,\n",
    "                    dropout=0.,\n",
    "                    exploration_max=.02,\n",
    "                    exploration_min=exploration_min,\n",
    "                    exploration_decay=exploration_decay,\n",
    "                    double_dq=True,\n",
    "                    pretrained=pretrained,\n",
    "                    run_id=run_id,\n",
    "                    n_actions=n_actions)\n",
    "\n",
    "\n",
    "# num_episodes = 10\n",
    "env.reset()\n",
    "total_rewards = []\n",
    "total_info = []\n",
    "\n",
    "for ep_num in tqdm(range(num_episodes)):\n",
    "    state = env.reset()[-1] # take the final dimension of shape (4, 84, 84) leaving shape (84, 84) \n",
    "    state = torch.Tensor([state]).unsqueeze(0) # converts (1, 84, 84) to (1, 1, 84, 84)\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    while True:\n",
    "        # if not training_mode:\n",
    "            # show_state(env, ep_num)\n",
    "\n",
    "\n",
    "        two_actions_index = agent.act(state)\n",
    "\n",
    "        two_actions_vector = agent.cur_action_space[0, two_actions_index]\n",
    "\n",
    "        two_actions = vec_to_action(two_actions_vector.cpu()) # tuple of actions\n",
    "        \n",
    "        steps += 1\n",
    "        reward = 0\n",
    "        info = None\n",
    "        terminal = False\n",
    "        for action in two_actions: \n",
    "            if not terminal:\n",
    "                # compute index into ACTION_SPACE of our action\n",
    "                step_action = ACTION_TO_INDEX[action]\n",
    "\n",
    "                state_next, cur_reward, terminal, info = env.step(step_action)\n",
    "                total_reward += cur_reward\n",
    "                reward += cur_reward\n",
    "\n",
    "        state_next = torch.Tensor([state_next[-1]]).unsqueeze(0)\n",
    "        reward = torch.tensor([reward]).unsqueeze(0)        \n",
    "        terminal = torch.tensor([int(terminal)]).unsqueeze(0)\n",
    "        \n",
    "        if training_mode:\n",
    "            agent.remember(state, two_actions_index, reward, state_next, terminal)\n",
    "            agent.experience_replay()\n",
    "        \n",
    "        state = state_next\n",
    "        if terminal:\n",
    "            break\n",
    "\n",
    "    total_info.append(info)\n",
    "    total_rewards.append(total_reward)\n",
    "\n",
    "    if training_mode and (ep_num % ep_per_stat) == 0:\n",
    "        save_checkpoint(agent, total_rewards, total_info, run_id)\n",
    "\n",
    "    with open(f'total_reward-{run_id}.txt', 'a') as f:\n",
    "        f.write(\"Total reward after episode {} is {}\\n\".format(ep_num + 1, total_rewards[-1]))\n",
    "        if (ep_num%100 == 0):\n",
    "            f.write(\"==================\\n\")\n",
    "            f.write(\"{} current time at episode {}\\n\".format(datetime.datetime.now(), ep_num+1))\n",
    "            f.write(\"==================\\n\")\n",
    "        #print(\"Total reward after episode {} is {}\".format(ep_num + 1, total_rewards[-1]))\n",
    "        num_episodes += 1\n",
    "\n",
    "if training_mode:\n",
    "    save_checkpoint(agent, total_rewards, run_id)\n",
    "\n",
    "env.close()\n",
    "fh.close()\n",
    "\n",
    "if num_episodes > ep_per_stat:\n",
    "    plt.title(\"Episodes trained vs. Average Rewards (per 500 eps)\")\n",
    "    plt.plot([0 for _ in range(ep_per_stat)] + \n",
    "                np.convolve(total_rewards, np.ones((ep_per_stat,))/ep_per_stat, mode=\"valid\").tolist())\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(agent.n_actions):\n",
    "    two_actions_vector = agent.cur_action_space[0, i]\n",
    "    two_actions = vec_to_action(two_actions_vector.cpu()) # tuple of actions\n",
    "    print(two_actions)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
