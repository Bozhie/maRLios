{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import gym\n",
    "import gym_super_mario_bros\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "from gym_super_mario_bros import SuperMarioBrosEnv\n",
    "from tqdm import tqdm\n",
    "import pickle \n",
    "import gym\n",
    "import numpy as np\n",
    "import collections \n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import datetime\n",
    "import json\n",
    "from toolkit.gym_env import *\n",
    "from toolkit.action_utils import *\n",
    "from toolkit.marlios_model import *\n",
    "from toolkit.train_marlios import *\n",
    "from toolkit.constants import *\n",
    "from toolkit.train_test_samples import *\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwitz\u001b[0m (\u001b[33mmarlios\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/cameronwitz/Desktop/CSCI_566/maRLios/wandb/run-20230425_195004-gc4kz1fw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/marlios/my-awesome-project/runs/gc4kz1fw' target=\"_blank\">pious-bird-43</a></strong> to <a href='https://wandb.ai/marlios/my-awesome-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/marlios/my-awesome-project' target=\"_blank\">https://wandb.ai/marlios/my-awesome-project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/marlios/my-awesome-project/runs/gc4kz1fw' target=\"_blank\">https://wandb.ai/marlios/my-awesome-project/runs/gc4kz1fw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/1000 [01:55<7:57:51, 28.79s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train(\n\u001b[1;32m      2\u001b[0m     name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mTesting Generalization first go\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      3\u001b[0m     training_mode\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, \n\u001b[1;32m      4\u001b[0m     pretrained\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, \u001b[39m# use the pretrained model\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m     ep_per_stat\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, \n\u001b[1;32m      6\u001b[0m     gamma\u001b[39m=\u001b[39;49m\u001b[39m0.9\u001b[39;49m,\n\u001b[1;32m      7\u001b[0m     num_episodes\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m,\n\u001b[1;32m      8\u001b[0m     run_id\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m      9\u001b[0m     lr\u001b[39m=\u001b[39;49m\u001b[39m0.00001\u001b[39;49m,\n\u001b[1;32m     10\u001b[0m     lr_decay\u001b[39m=\u001b[39;49m \u001b[39m0.999\u001b[39;49m,\n\u001b[1;32m     11\u001b[0m     exploration_min\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m,\n\u001b[1;32m     12\u001b[0m     exploration_max \u001b[39m=\u001b[39;49m \u001b[39m1\u001b[39;49m, \u001b[39m# setting this to the min for the rerun model\u001b[39;49;00m\n\u001b[1;32m     13\u001b[0m     exploration_decay\u001b[39m=\u001b[39;49m\u001b[39m0.995\u001b[39;49m, \n\u001b[1;32m     14\u001b[0m     action_space\u001b[39m=\u001b[39;49mTRAIN_SET,\n\u001b[1;32m     15\u001b[0m     n_actions\u001b[39m=\u001b[39;49m\u001b[39m64\u001b[39;49m,\n\u001b[1;32m     16\u001b[0m     debug\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     17\u001b[0m     device\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mmps\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     18\u001b[0m     max_time_per_ep\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m \u001b[39m# limit runs to 200 seconds\u001b[39;49;00m\n\u001b[1;32m     19\u001b[0m     )\n\u001b[1;32m     21\u001b[0m \u001b[39m# train(\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[39m#     name=\"Lowering min lr by a factor of ten\",\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[39m#     training_mode=True, \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[39m#     max_time_per_ep=150 # limit runs to 200 seconds\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[39m#     )\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/CSCI_566/maRLios/toolkit/train_marlios.py:195\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(training_mode, pretrained, lr, gamma, exploration_decay, exploration_min, ep_per_stat, exploration_max, lr_decay, mario_env, action_space, num_episodes, run_id, n_actions, debug, name, max_time_per_ep, device)\u001b[0m\n\u001b[1;32m    192\u001b[0m         total_reward \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m cur_reward\n\u001b[1;32m    193\u001b[0m         reward \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m cur_reward\n\u001b[0;32m--> 195\u001b[0m state_next \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mTensor([state_next])\n\u001b[1;32m    196\u001b[0m reward \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([reward])\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)        \n\u001b[1;32m    197\u001b[0m terminal \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([\u001b[39mint\u001b[39m(terminal)])\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(\n",
    "    name=\"Testing Generalization first go\",\n",
    "    training_mode=True, \n",
    "    pretrained=False, # use the pretrained model\n",
    "    ep_per_stat=10, \n",
    "    gamma=0.9,\n",
    "    num_episodes=1000,\n",
    "    run_id=None,\n",
    "    lr=0.00001,\n",
    "    lr_decay= 0.999,\n",
    "    exploration_min=0.2,\n",
    "    exploration_max = 1, # setting this to the min for the rerun model\n",
    "    exploration_decay=0.995, \n",
    "    action_space=TRAIN_SET,\n",
    "    n_actions=64,\n",
    "    debug=True,\n",
    "    device='mps',\n",
    "    max_time_per_ep=100 # limit runs to 200 seconds\n",
    "    )\n",
    "\n",
    "# train(\n",
    "#     name=\"Lowering min lr by a factor of ten\",\n",
    "#     training_mode=True, \n",
    "#     pretrained=True, # use the pretrained model\n",
    "#     ep_per_stat=100, \n",
    "#     gamma=0.9,\n",
    "#     num_episodes=3000,\n",
    "#     run_id='1682283822',\n",
    "#     lr=1e-8,\n",
    "#     lr_decay= 0.999,\n",
    "#     exploration_min=0.02,\n",
    "#     exploration_max = 0.02, # setting this to the min for the rerun model\n",
    "#     exploration_decay=0.995, \n",
    "#     action_space=SIMPLE_MOVEMENT,\n",
    "#     n_actions=len(SIMPLE_MOVEMENT),\n",
    "#     debug=True,\n",
    "#     device='mps',\n",
    "#     max_time_per_ep=150 # limit runs to 200 seconds\n",
    "#     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only need to run if you cut the above off early, even then you don't 'have' to but it will do it automatically for you, it's just not as nice to have the output above as well\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cp ModelCheckpoints/*1682283822* . resetting this run here\n",
    "\n",
    "run_id='1682283822'\n",
    "visualize(run_id=run_id, action_space=SIMPLE_MOVEMENT, n_actions=len(SIMPLE_MOVEMENT))\n",
    "# total_rewards = load_item('total_rewards-1682227242.pkl')\n",
    "# plot_rewards(ep_per_stat=100, total_rewards=total_rewards)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
